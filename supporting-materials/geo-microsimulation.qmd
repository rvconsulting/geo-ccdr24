---
title: "Georgia CCDR Microsimulation"
author:
  - name: "Renato Vargas"
    id: rv
    email: renovargas@gmail.com
    affiliation: 
      - name: Consultant for The World Bank
            
format:
  html:
    toc: true
    number-sections: true
    number-depth: 3
    highlight-style: github
  # docx:
  #   toc: true
  #   number-sections: true
  #   highlight-style: arrow
  # pdf:
  #   toc: true
  #   number-sections: true
  #   colorlinks: true
editor: source
editor_options: 
  chunk_output_type: console
bibliography: references.bib
csl: apa-6th-edition.csl
---

# Introduction

In this calculation file, we "age" the Georgian household survey according to demographic projections and different macroeconomic scenarios to explore the impact of climate-related risks and policy measures on the consumption expenditure distribution. It is part of a larger project with all background contributions to Georgia's CCDR, [available in this repository](https://rvconsulting.github.io/geo-ccdr24/supporting-materials/geo-microsimulation.html).

Using RStudio project makes it possible to not use `setwd()` to establish the root directory and refer to subdirectories in a relative manner, making interoperability easier within teams and not hard coding a particular computer's file structure into the code. If you are using Positron or Visual Studio Code (and the Quarto extension) just "open folder" at the root of the repository. If you are using R directly, just add `setwd(r'(C:\My\path\to\project\root)')` at the beginning of your coding session.

## Preamble

We start with a clean environment, making sure that any objects from a previous session are not present. We keep our country ISO code in a variable `iso` in case we need it later.

```{r}
#| warning: false
#| message: false

# Clean workspace
rm(list = ls())

# Georgia country ISO code
iso <- "GEO"

# Survey year
survey_year <- 2023

# # Exchange rate USD per GEL (Multiply by this to show results in USD)
# er <- 0.37

# Exchange rate 2017 (PPP) 2.6280 GEL per 1 US$ 

# Poverty lines
pline0 <- 212.8149 # Official USD3.60/Person/Day
# Using Georgia CPI, the 6.85 line would be (2023) GEL 355.10/person/month
# Using $3.60 in email, the 6.85 line would be (2023) GEL 404.9395
pline61 <- 355.10 # Intl. Poverty line of USD6.85/Person/Day
pline62 <- 404.9395 # Intl. Poverty line of USD6.85/Person/Day

# Poverty line used in the script below
pline <- pline0

# Years of interest for our macroeconomic scenario analysis
analysis_years <- c(2030, 2050)
```

We call the appropriate libraries.

Rather than calling our libraries as we go, we will make sure we have everything we need from the beginning.

```{r}
#| output: false
#| lst-label: lst-load-packages

library(tidyverse) # includes dplyr, ggplot2, purr...
library(haven)     # to read SPSS and Stata datasets
library(readxl)    # to read from MS-Excel
library(openxlsx)  # to write to MS-Excel.
library(gt)        # pretty tables
library(car)       # companion to applied regression
library(modelr)    # regression models
#library(anesrake)  
# Raking reweighting but we don't load it, because 
# it changes the meaning of summarize from dplyr, 
# so we use the form anesrake::anesrake() when using it.
library(janitor)   # pretty subtotals
library(broom)     # More regressions
library(zoo)       # Calculate moving window average and max value
# library(Hmisc)     # Estimates deciles, quintiles but use :: version too
# library(ineq) # Inequality measures
# library(acid) # Inequality measures we use acid::weighted.gini()

# Geopackages
library(sf)        # to read and write shapefile maps
library(terra)     # to perform geocalculations
library(tmap)      # for static and interactive maps
```

## Datasets

We then load the datasets that we need for this study. These are based on Georgia's Integrated Living Conditions Survey 2022 [@geostat_integrated_2023]. We make a note that the household identification variable is `UID`.

```{r}
#| warning: false
#| message: false
#| output: false
#| lst-label: original-datasets

## Household Unique ID, Weights, Location and other basic variables
hh_basics <- read_sav(
  "data/ilcs_2023/sysschedule.sav") |>
  mutate(
    UID = as.integer(UID))

# Household size (includes no. of family members)
hh_size <- read_sav(
  "data/ilcs_2023/familysize.sav")|> 
  mutate(
    UID = as.integer(UID))

# Processed income at household level
hh_income <- read_sav(
  "data/ilcs_2023/tblincomes.sav")|> 
  mutate(
    UID = as.integer(UID))

# Consumption aggregate at household level 
hh_expenditure <- read_sav(
  "data/ilcs_2023/tblexpenditures.sav")|> 
  rename(# rename total expenditure variables
         total_expenditure = MTlianixarjebi_,
         total_expenditure_aeq06 = MTlianimoxmareba_EqAdScale,
         total_expenditure_aeq08 = Mtlianimoxmareba_EqAdScale_08) |> 
  mutate(
    UID = as.integer(UID))

# Characteristics of the dwelling
hh_chars <- read_sav(
  "data/ilcs_2023/tblshinda01.sav")|>
  mutate(
    UID = as.integer(UID))

# Persons (pp)
pp <- read_sav(
  "data/ilcs_2023/tblshinda02.sav") |> 
  mutate(
    UID = as.integer(UID),
    MemberNo = as.integer(MemberNo))

# Labor (pp)
pp_labor <- read_sav(
  "data/ilcs_2023/tblshinda05_1.sav") |> 
  mutate(
    UID = as.integer(UID),
    MemberNo = as.integer(MemberNo),
    Q5  = as.integer(Q5),
    Q12 = as.integer(Q12)) 

# Poverty
poverty <- read_dta(
  "data/ilcs_2023/POVERTY_stata.dta") |> 
  rename(official_pline = pline) |> 
  mutate(
    UID = as.integer(UID),
    poor_2023 = if_else(aecons < pline, 1, 0))

# Ind. Poverty
ind_poverty <- read_dta(
  "data/ilcs_2023/IND_POVERTY_stata.dta") |> 
  rename(MemberNo = memberno) |> 
  mutate(
    UID = as.integer(UID),
    MemberNo = as.integer(MemberNo))

# Food diary
food_q <- read_sav(
  "data/ilcs_2023/tblconsumption.sav") |> 
  rename(UID = UID)

food_price <- read_sav( 
  "data/ilcs_2023/tblavgprices.sav")

# Maps
adm1 <- sf::read_sf("data/gis/geo-adm1.shp") |>
  dplyr::select(RegNo, region, ADM1_PCODE, ADM1_EN, ADM1_KA, geometry) |>
  dplyr::arrange(ADM1_PCODE)

regions <- as.data.frame(adm1) |> 
  select(-geometry)
```

We also need look-up tables.

```{r}
#| warning: false
#| message: false
#| label: look-up-tables

sam_activities <- read_excel(
    "data/sam/classifications.xlsx",
    sheet = "SAM-REV2",
    col_names = T,
    col_types = c("text", "text", "text","text", "numeric")
  )

sam_factors <- read_excel(
    "data/sam/classifications.xlsx",
    sheet = "SAM factors",
    col_names = T,
    col_types = "text",
  )

coicop <- read_excel(
    "data/sam/classifications.xlsx",
    sheet = "COICOP",
    col_names = T,
    col_types = "text",
  ) |> 
  mutate(simple_code = as.integer(gsub("\\.", "", Coicop)))

coicop_filtered <- coicop |> 
  filter( nchar(as.character(simple_code)) >= 5)

```


We also have Continuous Labor Survey data at the individual level, which will come in handy if we do not get access to the labor part of the ILCS. See data folder for documents describing the datasets.

```{r}
#| warning: false
#| message: false
#| output: false
#| lst-label: labor-survey

# Labor Force Survey
lfs_2023 <- read_sav(
  "data/lfs_2023/LFS_ECSTAT_ENG_2023.sav") |> 
  rename(UID = UID)

# Labor Force Survey Demographic Characteristics
lfs_2023_dem <- read_sav(
  "data/lfs_2023/LFS_Demographic_ENG_2023.sav") |> 
  rename(UID = UID)

```

We will work non-destructively, meaning we will not rewrite these data sets and we will only create intermediate data frame objects from them to perform transformations, selections and other data management tasks. For example, we will keep household assignment to poverty status and consumption deciles handy by creating a subset of our `hh_expenditure` data with only our household identifiers, deciles, and poverty if available.

```{r}
#| lst-label: lst-deciles
#| warning: FALSE
#| message: FALSE

# We will estimate deciles from consumption
deciles <- hh_expenditure |> 
  select( 
    # Keep household id and expenditure variables
    UID, 
    total_expenditure,
    total_expenditure_aeq06, # Adult equivalent * 0.6
    total_expenditure_aeq08) # Adult equivalent * 0.8
```

Our population data comes from UN's projections.

```{r}
#| warning: FALSE
#| message: FALSE 
#| lst-label: lst-population-projections

population_projections <- read_dta("data/population/UN2022_population.dta") |> 
  filter(country == iso) # we filter for Georgia
```

The macro scenario dataset is an input provided by the Macroeconomic CGE simulation team, with yearly information on GDP, working age population, employment by economic activity (for an aggregation of three sectors: agriculture, manufacturing, and services), wages by economic activity, value added by economic activity, remittances, consumer price index, food price index and energy price index (for a bundle of gas, oil, coal, electricity) by decile (10 representative households in the macro model), and carbon tax revenue transfers to household deciles.

```{r}
#| warning: false
#| message: false
#| output: false
#| lst-label: lst-import-macro-scenarios

scenario_file <- "data/sam/MacroScenarioInformation_GEO.xlsx"
scenario_varlist <- read_xlsx("data/sam/GEO_Macro_varlist.xlsx") |> 
  select(-category)
# prices_2030 <-
#   read.csv("data/ARM-Microsimulation/prices2030.csv")
```

Economic Activities in the Survey are in Georgian. The following dataset is a lookup table with the English names.

```{r}
#| eval: false
#| lst-label: lst-import-economic-activity-codes

# Equivalence table
sectors <- read_excel(
    "data/sam/classifications.xlsx",
    sheet = "SAM-REV2",
    col_names = T,
    col_types = "text",
  )
```

We also have an Excel file with changes to labor productivity due to climate variability, by sector. Also, we have livestock productivity changes.

```{r}
#| warning: false
#| message: false
#| output: false
#| lst-label: lst-import-labor-productivity

labor_productivity <-
  read.csv(
    "data/climate_productivity/GEO_labour_REF_shock_admin1_bySector.csv")

agriculture_productivity <-
  read.csv(
    "data/climate_productivity/GEO_agriculture_revenue_impact_GCM-Historical.csv"
  )

livestock_productivity <-
  read.csv(
    "data/climate_productivity/GEO_livestock_REF_shock_admin1_high_prop_holstein.csv"
  )
```

And finally, direct effects on consumption of Macro scenarios.

```{r}
# HH Consumption Macro Effect
hh_consumption_effects <- read_excel(
    "data/sam/MacroconsumptionEffect_HH_GEO.xlsx",
    sheet = "HH_consumption_value",
    col_names = F,
    range = "A90:AD119"
  )
names(hh_consumption_effects) <- c("scenario_id", "hh_type",c(2023:2050))
hh_consumption_effects <- hh_consumption_effects |> 
  mutate(
    scenario_id = case_when(
      scenario_id == "ccdr_all_dry_hot" ~ "dryhot",
      scenario_id == "ccdr_all_wet_warm" ~ "wetwarm",
      scenario_id == "ccdr_NZS_10_10" ~ "nzs",
      .default = scenario_id
    )) |> 
  select(-3)

```



# Data preparation, demographic characteristics, income outliers and missings

We start with various renames for standardization. Naming conventions in the guidance code use traditional abbreviations like `nli` for non-lablor income. We are opting for more descriptive variable names like `non_labor_income`, `labor_income`, etc. to have more easily readable code. We make an exception for total consumption (`tc`), because it's a variable that we use in every scenario and it supersedes lenght limits when adding scenario identifiers.

```{r}
# Uncomment the correct total expenditure variable below
ex <- hh_expenditure |> 
  rename(
    tc =
      total_expenditure
      #total_expenditure_aeq06 # Adult equivalent * 0.6
      #total_expenditure_aeq08 # Adult equivalent * 0.8
      )
```

We extract demographic characteristics for each individual.

## Skill level

For skill level, we will use information on schooling from `pp$Education` (`TblShinda02`), which has the following levels:

1.  Illiterate
2.  Do not have primary education but can read and write
3.  Pre-primary education
4.  Primary education
5.  Lower secondary education
6.  Upper secondary education
7.  Vocational education without secondary general education
8.  Vocational education on the base of lower secondary education with secondary general education certificate
9.  Vocational education on the base of secondary general education (except higher professional education)
10. Higher professional program
11. Bachelor or equivalent
12. Master or equivalent
13. Doctor or equivalent

We need three skill levels for our SAM template, so we map these levels to:

Low skill (1 - 5): Illiterate through lower secondary.
Medium skill (6 - 9): Upper secondary through vocational education.
High skill (10 - 13): Higher professional program through Doctor.


```{r}
#| warning: false
#| message: false
#| label: skill-level

pp_factor_descriptors <- pp |>
  select(UID, MemberNo, Gender, Age, Education) |> 
  mutate(
    MemberId = 
      paste0(sprintf("%06d", UID), sprintf("%02d", MemberNo))) |>
  mutate(Gender = factor(
    Gender,
    levels = c(1, 2),
    labels = c("Female", "Male")
  )) |> 
  mutate(
    SkillLevel = case_when(
      Education >= 0 & Education <= 5 ~ 1,
      Education > 5 & Education <= 9 ~ 2,
      Education > 9 & Education <= 13 ~ 3,
      TRUE ~ NA ) ) |> 
  mutate(
    SkillLevel = factor(
      SkillLevel, 
      levels = c( 1, 2, 3),
      labels = c( "Low Skill", "Medium Skill", "High Skill"))
  )
```

Now that we have skill levels, we need to add information on urban/rural (from `hh_basics`) and quintile (from `ind_poverty`), and type of income earner (from `pp_labor`).

```{r}
#| warning: false
#| message: false
#| label: factor-income-descriptors

urb_rur <- hh_basics |> 
  select(UID,QuartNo, UrbanOrRural, RegNo, Weights) |> 
  mutate(
    UrbanOrRural = factor(
      UrbanOrRural,
      levels = c(2,1),
      labels = c("Rural", "Urban")
    )
  )

quintiles <- poverty |> 
  select(UID, quintilc, decilc, hhsize) |> 
  rename(
    Quintile = quintilc,
    Decile = decilc) |> 
  mutate(
    Quintile = factor(
      Quintile,
      levels = c(1:5),
      labels = c("Q1", "Q2", "Q3", "Q4", "Q5")
    ),
    Decile = factor(
      Decile,
      levels = c(1:10),
      labels = c(
        "D01", "D02", "D03", "D04", "D05",
        "D06", "D07", "D08", "D09", "D10")
  ))

is_employed <- ind_poverty |> 
  mutate(
    MemberId = 
      paste0(sprintf("%06d", UID), sprintf("%02d", MemberNo))) |> 
    mutate(
      employed = case_when(
        empl == 1 ~ T,
        empl == 0 ~ F,
        .default = NA
      )
    ) |> 
    select(
      MemberId, employed
    )

pp_lmarket0 <- pp_labor |> 
  mutate(
    MemberId = 
      paste0(sprintf("%06d", UID), sprintf("%02d", MemberNo))) |> 
  select(-c(UID,MemberNo))

pp_lmarket1 <- pp_factor_descriptors |> 
  left_join(urb_rur, join_by(UID)) |> 
  left_join(quintiles, join_by(UID)) |> 
  left_join(pp_lmarket0, join_by(MemberId)) |> 
  relocate(c(UID, MemberNo, MemberId, QuartNo), .before = 1)
    
```

## Labor status and Economic Activities

We work with labor status from `Shinda05_1`. Since, upon import NACE 2 codes are converted to numbers, we need to convert them back to text, so that we can keep zeros to the left for proper order. We then extract the first two digits and find the correspondence to Rev. 2 from the SAM using the look-up table `sam_activities`. For proper order, we convert the SAM activities columns for job 1 and job 2 to factor, using the order from the dataset `sam_factors`.

```{r}
#| warning: false
#| message: false
#| label: nace-codes

pp_microsim01 <- pp_lmarket1 |> 
  mutate(
    MemberId = 
      paste0(sprintf("%06d", UID), sprintf("%02d", MemberNo))) |>
  mutate(
    # Job 1 NACE Rev 2 code. 
    Q5  = if_else(!is.na(Q5),paste0(sprintf("%04d", Q5)), NA),
    # Job 2 NACE Rev 2 code.
    Q12 = if_else(!is.na(Q12),paste0(sprintf("%04d", Q12)), NA)) |> 
  mutate(
    job1 = if_else(!is.na(Q5),substr(Q5, 1, 2), NA),
    job2 = if_else(!is.na(Q12),substr(Q12, 1, 2), NA)
  ) |> 
  # Is employed?
  left_join(
    is_employed,
    join_by(MemberId)
  ) |> 
  # We match to Rev 2 and SAM classifications (for job 1 and job 2)
  left_join(
    sam_activities[,c(1,5)], 
    join_by(job1 == rev2_2d)) |> 
  left_join(
    sam_activities[,c(1,5)],
    join_by(job2 == rev2_2d),
    suffix = 
      c("_job1", "_job2")) |> 
  # And convert to factors for proper order
  mutate(
    SAM3_job1 = factor(
      SAM3_job1, 
      levels = c(1:3),
      labels = c("Agriculture", "Manufactures", "Services")
      ),
    SAM3_job2 = factor(
      SAM3_job2, 
      levels = c(1:3),
      labels = c("Agriculture", "Manufactures", "Services")
      )
  ) 
```

## Types of income

Before making our multi-dimensional tables, we need to identify different types of income. f-lab (wages) and f-surp (capital income). The instruction is that f-surp needs to be split into wages to entrepreneurs/self employed and capital income.

```{r}
#| warning: false
#| message: false
#| label: identify-incomes

pp_microsim02 <- pp_microsim01 |> 
  mutate(
    # We add accross three months for each source (and coalesce the NAs to 0)
    labor_income_job1 = 
      rowSums(
        across(starts_with("Q8_faqti_"), \(x) coalesce(x, 0))),
    labor_income_job2 = 
      rowSums(
        across(starts_with("Q14_faqti_"), \(x) coalesce(x, 0))),
    surplus_income = 
      rowSums(
        across(starts_with("Q10_faqti_"), \(x) coalesce(x, 0)))
  ) |> 
  # We also add factor labels to Employment Status
  mutate(
    lstatus1 = factor(
      Q7,
      levels = c(1:6),
      labels = c(
        "Employee", "Employer", "Own Account (Non-Ag.)", 
        "Own Account (Ag.)", "Unpaid Worker", "Other"))
    ) |> 
  mutate(
    lstatus2 = factor(
      Q13,
      levels = c(1:6),
      labels = c(
        "Employee", "Employer", "Own Account (Non-Ag.)", 
        "Own Account (Ag.)", "Unpaid Worker", "Other"))
    )
```


## Missing and outliers

In this section we will assign a labor income for job1 holders with `!lstatus1 %in% c(2,5) & labor_income_job1 == 0` based on predicted income from everyone else who doesn't meet the condition. We will estimate annual_labor_total after predictions.

Looking at the data we see that only those that report being an employee or "other" report having labor income 1 or 2; Employer and Own Account non-ag report having surplus; and own account ag and (of course) unpaid worker

We first identify who needs predictions for job1, job2, and surplus. We default to `NA` because we want to preserve the logic of those who don't have an income, because they aren't supposed to have one. However, this introduces an uncertainty element when predicting further down the line, because subsetting does not allow NA's. Even if we want to match just `TRUEs`. So it's a double-edged sword. The fix was using `which()` to find row numbers of those with `TRUE`. 

```{r}
pp_microsim03 <- pp_microsim02 |> 
  mutate(
    fix_job1 = case_when(
      (!is.na(Q7) & Q7 %in% c(2,3,4,5)) ~ F,
      (!is.na(Q7) & Q7 %in% c(1,6) & labor_income_job1 >  0) ~ F,
      (!is.na(Q7) & Q7 %in% c(1,6) & labor_income_job1 == 0) ~ T,
      .default = NA
    ),
    fix_job2 = case_when(
      (!is.na(Q13) & Q13 %in% c(2,3,4,5)) ~ F,
      (!is.na(Q13) & Q13 %in% c(1,6) & labor_income_job2 >  0) ~ F,
      (!is.na(Q13) & Q13 %in% c(1,6) & labor_income_job2 == 0) ~ T,
      .default = NA
    ),
    fix_surplus = case_when(
      (!is.na(Q7) & Q7 %in% c(1,4,5,6)) ~ F,
      (!is.na(Q7) & Q7 %in% c(2,3) & surplus_income >  0) ~ F,
      (!is.na(Q7) & Q7 %in% c(2,3) & surplus_income == 0) ~ T,
      .default = NA
    ))
```

Outliers and need to predict.

```{r}
pp_microsim04 <- pp_microsim03 |> 
  mutate(
    sd_job1    = sd(labor_income_job1, na.rm = T),
    sd_job2    = sd(labor_income_job2, na.rm = T),
    sd_surplus = sd(surplus_income   , na.rm = T),
    d_job1 = labor_income_job1 / sd_job1,
    d_job2 = labor_income_job2 / sd_job2,
    d_job1 = surplus_income / sd_surplus,
  )
```

Assign sector to missings.

```{r}
pp_microsim05 <- pp_microsim04 |>
  group_by(UID) |>
  mutate(
    # Create a temporary variable 'other_sector' as a factor
    other_sector_job1 = case_when(
      !is.na(Q7) & !is.na(SAM3_job1) & SAM3_job1 %in% levels(SAM3_job1) ~ SAM3_job1,
      TRUE ~ NA_character_ # Keep as character NA for now
    )
  ) |>
  fill(other_sector_job1, .direction = "downup") |>
  mutate(
    other_sector_job1 = if_else(is.na(Q7), NA_character_, other_sector_job1)
  ) |>
  mutate(
    # Impute missing 'sector' values based on 'other_sector'
    SAM3_job1 = as.factor(if_else(
      !is.na(Q7),
      as.character(coalesce(as.character(SAM3_job1), other_sector_job1)),
      as.character(SAM3_job1)
    )),
    SAM3_job2 = as.factor(if_else(
      !is.na(Q13),
      as.character(coalesce(as.character(SAM3_job2), other_sector_job1)),
      as.character(SAM3_job2)
    )),
    # Re-establish levels and labels
    SAM3_job1 = factor(SAM3_job1, levels = c("Agriculture", "Manufactures", "Services")),
    SAM3_job2 = factor(SAM3_job2, levels = c("Agriculture", "Manufactures", "Services"))
  ) |>
  ungroup()
```

## The income simulation regression

Since labor income was a key variable, which we needed to match with the future wage bill by economic activity, we first checked for missing values among employed individuals. We found that almost a third of respondents (28.6%) did not report income for either their primary or secondary job. To overcome this limitation, we used the available information from the remaining respondents to estimate an extended Mincer equation, as shown in @eq-labor-income-regression, and implemented in @lst-regression-model. For the respondents with available information, we also identified outliers as those outside of five standard deviations from the mean labor income.

$$
\begin{equation}
\begin{split}
\ln(lab_i) = \\ \beta_0 + \beta_1 \text{age}_i + \\
\beta_2 \text{gender}_i + \beta_3 \text{education}_i + \\ 
\beta_4 \text{age}^2_i + \beta_5 \text{marz}_i + \\
\beta_6 \text{sector}_i + \epsilon_i
\end{split}
\end{equation}
$$ {#eq-labor-income-regression}

Where:

-   $\ln(lab_i)$ is the natural logarithm of labor income for individual $i$.
-   $\beta_0$ is the intercept term.
-   $\beta_1, \beta_2, \beta_3, \beta_4, \beta_5, \beta_6$ are the coefficients for the respective independent variables.
-   $\text{age}_i$ is the age of individual $i$.
-   $\text{gender}_i$ is a binary variable indicating the gender of individual $i$ (1 for female, 2 for male).
-   $\text{education}_i$ represents the level of education for individual $i$ (ordered: 1) None to General, 2) Secondary to Vocational, 3) Higher education).
-   $\text{age}^2_i$ is the square of the age of individual $i$, included to capture non-linear effects of age on labor income.
-   $\text{marz}_i$ represents the region where individual $i$ resides.
-   $\text{sector}_i$ represents the sector of employment for individual $i$ (i.e., agriculture, manufacturing or services).
-   $\epsilon_i$ is the error term for individual $i$.

We first prepare our variables for the regression.

```{r}
pp_microsim06 <- pp_microsim05 |>
  rename(
    education = Education,
    age = Age,
    gender = Gender,
    region = RegNo) |> 
  mutate(
    education2 = education^2,
    age2 = age^2,
    male = case_when(
      gender == 1 ~ 1,
      gender == 2 ~ 0
    ),
    ln_lab1 = if_else(
      !is.na(labor_income_job1) & labor_income_job1 != 0,
      log(labor_income_job1),
      NA),
    ln_lab2 = if_else(
      !is.na(labor_income_job2) & labor_income_job2 != 0,
      log(labor_income_job2),
      NA),
    ln_surplus = if_else(
      !is.na(surplus_income) & surplus_income != 0,
      log(surplus_income),
      NA),
    sim_job1 = NA_real_,
    sim_job2 = NA_real_,
    sim_surplus = NA_real_
  )|>
  # Labor Market Status 
  mutate(
    lmarket = case_when(
      !is.na(Q7) ~ as.numeric(SAM3_job1),
      is.na(Q7) & age >= 15 ~ 4, # Unemployed
      is.na(Q7) & age < 15 ~ 5,  # OLF
      .default = NA_integer_
  )
)

```

Filter the data for regression conditions.

```{r}
#| eval: true
regression_data_job1 <- pp_microsim06 |>
  filter(Q7 %in% c(1,6) & fix_job1 == F)

regression_data_job2 <- pp_microsim06 |> 
  filter(Q13 %in% c(1,6) & fix_job2 == F)

regression_data_surplus <- pp_microsim06 |> 
  filter(Q7 %in% c(2,3) & fix_surplus == F)
```

Regression model.

```{r}
#| eval: true
#| lst-label: lst-regression-model
#| lst-cap: "Income regression model"

model_job1 <- lm(
  ln_lab1 ~ 
    age + gender + education + 
    age2 + region + SAM3_job1,
    data = regression_data_job1)

model_job2 <- lm(
  ln_lab2 ~ 
    age + gender + education + 
    age2 + region + SAM3_job2,
    data = regression_data_job2)

model_surplus <- lm(
  ln_surplus ~ 
    age + gender + education + 
    age2 + region + SAM3_job1,
    data = regression_data_surplus)
```

Applying predictions to those who need it.

Note: The 'predict' function in R does not directly support conditions within the function call, so we handle this by filtering or subsetting the data as needed.

Note: 'type = "response"' might be needed depending on model type.

```{r}
# rows to predict (this removes uncertainty NAs for predictions)
target_rows_job1    <- which(pp_microsim06$fix_job1    == TRUE)
target_rows_job2    <- which(pp_microsim06$fix_job2    == TRUE)
target_rows_surplus <- which(pp_microsim06$fix_surplus == TRUE)

# predictions
pp_microsim06$sim_job1[target_rows_job1] <- exp(
  predict(
    model_job1, 
    pp_microsim06[target_rows_job1, ], 
    type = "response")
)

pp_microsim06$sim_job2[target_rows_job2] <- exp(
  predict(
    model_job1, 
    pp_microsim06[target_rows_job2, ], 
    type = "response")
)

pp_microsim06$sim_surplus[target_rows_surplus] <- exp(
  predict(
    model_job1, 
    pp_microsim06[target_rows_surplus, ], 
    type = "response")
)
```

At this point, if there were negative predictions, we would have to make them zero. There are none such cases in this exercise.

And now, we replace simulated income for those who lack one.

```{r}
pp_microsim07 <- pp_microsim06 |> 
  mutate(
    labor_income_job1 = if_else(
      fix_job1 == T,
      sim_job1,
      labor_income_job1
    ),
    labor_income_job2 = if_else(
      fix_job2 == T,
      sim_job2,
      labor_income_job2
    ),
    surplus_income = if_else(
      fix_surplus == T,
      sim_surplus,
      surplus_income
    )
  )
```

Finally, we estimate total labor income.

```{r}
pp_microsim08 <- pp_microsim07 |> 
  mutate(
    # Annual income
    annual_labor_income_job1 = labor_income_job1 * 4,
    annual_labor_income_job2 = labor_income_job2 * 4,
    annual_surplus_income = surplus_income * 4,
    # Monthly income
    monthly_labor_income_job1 = labor_income_job1 / 3,
    monthly_labor_income_job2 = labor_income_job2 / 3,
    monthly_surplus_income = surplus_income / 3
  ) |>
  mutate(
    # Annual labor income in GEL
    annual_labor_total = if_else(
      Q7 %in% c(1,2,3,6) | Q13 %in% c(1,2,3,6),
      (coalesce(
        annual_labor_income_job1, 0) +
       coalesce(
        annual_labor_income_job2, 0) +
        coalesce(
        annual_surplus_income, 0)
      ),
      NA_real_
    ),
    # Monthly labor income in GEL
    monthly_labor_total = if_else(
      Q7 %in% c(1,2,3,6) | Q13 %in% c(1,2,3,6),
      (coalesce(
        monthly_labor_income_job1, 0) +
       coalesce(
        monthly_labor_income_job2, 0) +
        coalesce(
        monthly_surplus_income, 0)
      ),
      NA_real_
    ))
```

### Total income and shares

Total labor income at HH level.

```{r}
pp_microsim09 <- pp_microsim08 |>
  group_by(UID) |>
  mutate(
    hh_annual_labor_total  = sum(annual_labor_total,  na.rm = T),
    hh_monthly_labor_total = sum(monthly_labor_total, na.rm = T)
    ) |>
  ungroup()


```

# UN Population Projections

Now we are ready to move to our demographic projections and macroeconomic model information.

First, filtering based on country (our `iso` variable).

```{r}
population_projections <- population_projections  |>  
  filter(country == iso)
```

Collapsing data by summing up variables starting with "yf" and "ym" and reshaping data to long format.

```{r}
#| warning: false
#| message: false

population_projections <- population_projections |>
  group_by(Variant, country, cohort) |>
  summarize(across(starts_with(c("yf", "ym")), sum)) |>
  ungroup()

population_projections <- pivot_longer(population_projections,
                              cols = starts_with(c("yf", "ym")),
                              names_to = c(".value", "year"),
                              names_pattern = "(yf|ym)(.*)")
```

Creating new variable `total_population` as the sum of `yf` and `ym`. Dropping `country` variables.

```{r}

population_projections <- population_projections |>
  mutate(total_population = yf + ym) |>
  select( -country) |> 
  mutate(year = as.numeric(year))
```

Summarizing the year to find the range.

```{r}

minyear <- survey_year # Make sure `survey_year` is correctly defined
maxyear <- max(as.numeric(population_projections$year))
```

We have that the "Min Year" is `minyear` and the "Max Year" is `maxyear`. Now we create a population growth rate by demographic variant dataset. We initialize an empty list to store our data by variant and we loop over variants to create this list.

```{r}
# With minyear and maxyear defined above
# Initialize a list to store growth data
pop_growth <- list()

# Loop over variants
variants <- unique(population_projections$Variant)
for (variant in variants) {
  for (t in minyear:maxyear) {
    
    # Calculate population for year t
    pop_t <- population_projections |>
      filter(year == t, Variant == variant) |>
      summarize(sum_pop = sum(total_population)) |>
      pull(sum_pop)
    
    # Calculate population for base year
    pop_base <- population_projections |>
      filter(year == minyear, Variant == variant) |>
      summarize(sum_pop = sum(total_population)) |>
      pull(sum_pop)
    
    # Calculate growth rate and store in list with dynamic naming
    growth_rate <- pop_t / pop_base
    pop_growth[[paste0(t, "_", variant)]] <- list(
      growth_rate = growth_rate, pop_t = pop_t
      )
  }
}
```

With the list ready, we convert back to dataframe by stitching the list elements one on top of the other.

```{r}
# Convert list to dataframe
pop_growth <- do.call(rbind, lapply(names(pop_growth), function(x) {
  # Extract year and variant from the name
  parts <- unlist(strsplit(x, "_"))
  year <- as.integer(parts[1])
  variant <- parts[2]
  
  # Create a tibble for each entry
  tibble(year = year, 
         variant = variant, 
         total_population = pop_growth[[x]]$pop_t,
         pop_growth_rate = pop_growth[[x]]$growth_rate)
}))

# Arrange the dataframe for better readability
pop_growth <- arrange(pop_growth, variant, year)

# Display the first few rows of the dataframe
pop_growth[c(1:09),]
```

# Macro Scenarios

Here we use the Excel tab names to create the names of the scenarios going forward, with a previous cleaning in which we convert names to lower case, replace spaces and special characters with underscores, we remove the word scenario from the name, and remove leading or trailing spaces or underscores.

```{r}
#| output: false
#| warning: false
#| message: false

# Macro Scenario File imported in "Datasets" section (scenario_file) 
sheets <- excel_sheets(scenario_file)
scenario_sheets <- sheets[c(1:4)]

# Define the names of the scenarios and the variants
# modify list with the tab numbers in the Excel file
scenarios <- scenario_sheets |>
  # Convert all text to lowercase
  str_to_lower() |>  
  # Remove all spaces and hyphens
  # (we want lowercase one word scenario names for variable names)
  str_replace_all("[ -]", "") |>
  # Remove the word 'scenario' or 'scenarios'
  str_remove_all("scenario?s?") |>
  # Remove leading and trailing underscores
  str_replace_all("^_+|_+$", "")  

```

Our scenarios are: `{r} glue::glue_collapse(scenarios, sep = ', ', last = ', and ')`. We now import data from Excel sheets corresponding to each scenario and combine them into one data frame.

```{r}
#| output: false
#| warning: false
#| message: false

# Create an empty list to store data frames for each scenario
scen_data_list <- list()

# Import data for each scenario and store it in the list.
# Note the trick where we use the index `i` from `scenarios`
# but access the scenario_sheets name to fetch the Excel
# tab.
for (i in seq_along(scenarios)) {
  sheet_data <- read_excel(scenario_file, 
                           sheet = scenario_sheets[i], 
                           range = "A3:AX30",
                           col_names = FALSE)
  sheet_data$scenario_id <- scenarios[i]
  colnames(sheet_data) <- scenario_varlist$var_short_name
  scen_data_list[[i]] <- sheet_data
}

# Combine all data frames into one
macro_data <- bind_rows(scen_data_list)
# Remove unnecessary list
rm(scen_data_list)
```

We then rename columns, create a 'scenid' to identify scenarios, and merge with population projections. Calculate real wages

```{r}
# Rename population_m from the data set because we will use 
# UN pop projections from the other data set.
macro_data <- macro_data |> 
  rename(population_m_macrodata = population_m)

```

We prepare our population data to combine it with the macro data.

```{r}
pop_data <- population_projections |> 
  group_by(Variant, year) |> 
  summarize(female = sum(yf),
            male = sum(ym),
            total_population = sum(total_population) ) |> 
  ungroup()

# Filter population data to macro model years
pop_data <- pop_data |> 
  filter(year <= max(macro_data$year),
         Variant == variants[7])
# Merge the combined data with population projections
macro_data <- macro_data |>
  left_join(pop_data, by = c("year"))
```

There are some calculated variables that we need to estimate.

```{r}
# Calculate real wages
macro_data <- macro_data |>
  # Compute real wages
  mutate(
    rwage_agr_b_lcu = (wage_agri_lcu1000s * emp_agri_million) / cpi,
    rwage_man_b_lcu = (wage_manu_lcu1000s * emp_manu_million) / cpi,
    rwage_ser_b_lcu = (wage_serv_lcu1000s * emp_serv_million) / cpi
  ) |>
  # Define labor market groups using the macro-based population base
  mutate(
    lmarket_1 = emp_agri_million,
    lmarket_2 = emp_manu_million,
    lmarket_3 = emp_serv_million,
    lmarket_4 = working_age_pop_m - (lmarket_1 + lmarket_2 + lmarket_3),
    lmarket_5 = population_m_macrodata - working_age_pop_m
  )

```

We calculate columns for the totals by labor market group so we can derive our shares.

```{r}
# And we label these variables
attr(macro_data$total_population, 
     "label") <- "Total population (million)"
attr(macro_data$lmarket_1, 
     "label") <- "Employed in agriculture (million)"
attr(macro_data$lmarket_2, 
     "label") <- "Employed in manufacturing (million)"
attr(macro_data$lmarket_3, 
     "label") <- "Employed in services (million)"
attr(macro_data$lmarket_4, 
     "label") <- "Unemployed and inactive (million)"
attr(macro_data$lmarket_5, 
     "label") <- "Outside the labor force (million)"
```

With our demographic data added to our macroeconomic data, we need to estimate relative growth of some of the variables. For this we create a function to estimate growth per column to a named list of column.

```{r}
# Function to add growth rate columns directly in the dataframe
calculate_growth <- function(data, value_column) {
  growth_col_name <- paste0(value_column, "_growth") # dynamic name for growth column
  data |>
    arrange(year) |>
    group_by(Variant, scenario_id) |>
    mutate(
      base_value = first(!!sym(value_column)),
      !!sym(growth_col_name) := !!sym(value_column) / base_value
    ) |>
    select(-base_value) |> # optionally remove base_value column if not needed
    ungroup()
}

# Columns to calculate growth for
value_columns <- c(
  "rwage_agr_b_lcu",     # Real wage agriculture
  "rwage_man_b_lcu",     # Real wage manufacturing
  "rwage_ser_b_lcu",     # Real wage services
  "lmarket_1",
  "lmarket_2",
  "lmarket_3",
  "lmarket_4",
  "lmarket_5"
  )

```

We create the list and pass it to the function.

```{r}
# Using purrr to apply the function column-wise, without a for loop.
macro_data <- reduce(value_columns, calculate_growth, .init = macro_data)

# We relocate some variables for clarity.
macro_data <- macro_data |> 
  relocate(scenario_id, Variant, .before = year) |> 
  arrange(scenario_id, Variant, year)

```

Now that `macro_data` has growth rate columns for each of the variables. We can check, for example, the employment and wage growth rates for our three scenarios in the analysis years.


```{r}
#| label: tbl-lmarket-growth
#| tbl-cap: "Labor market growth by category in analysis years"

macro_data[macro_data$year %in% c(2030, 2050),c(
  c("scenario_id",
    "lmarket_1_growth",
    "lmarket_2_growth",
    "lmarket_3_growth",
    "lmarket_4_growth", 
    "lmarket_5_growth")
)] |> 
  gt(rowname_col = "scenario_id") |>
  cols_label(
    lmarket_1_growth = md("Agriculture"),
    lmarket_2_growth = md("Manufacturing"),
    lmarket_3_growth = md("Services"),
    lmarket_4_growth = md("Unemployed"),
    lmarket_5_growth = md("OLF")
  ) |>
  fmt_number(columns = everything(),
             decimals = 2)
```


```{r}
#| label: tbl-wage-growth
#| tbl-cap: "Wage bill growth by sector in analysis years"

macro_data[macro_data$year %in% c(2030, 2050),c(
  c("scenario_id",
    "rwage_agr_b_lcu_growth", 
    "rwage_man_b_lcu_growth",
    "rwage_ser_b_lcu_growth")
)] |> 
  gt(rowname_col = "scenario_id") |>
  cols_label(
    rwage_agr_b_lcu_growth = md("Agriculture"),
    rwage_man_b_lcu_growth = md("Manufacturing"),
    rwage_ser_b_lcu_growth = md("Services")
  ) |>
  fmt_number(columns = everything(),
             decimals = 2)
```

# Reweighting of the dataset

## Aggregation of population data

This is based on a custom command to reweight the survey according to macroeconomic data for every possible combination of variant, year, and country. In the macro data we know they only used the "medium" variant and we only need to reweight for two specific years for Georgia (GEO), so we will conduct the reweighting directly with these parameters.

We join several cohorts from 0 to 29 years old and from 60 onwards, because the reweighting procedure works best if each category is at least 5% of the population. The solution here works best for Georgia.

```{r}
population_projections <- population_projections |>
  # filter(Variant == "Medium") |>
  # Recoding cohorts into ordered factors
    mutate(
      cohort_short = factor(
        case_when(
          cohort %in% 
            c("P0004", "P0509","P1014",
              "P1519","P2024", "P2529") ~ "P0029",
          cohort %in% 
            c("P3034", "P3539") ~ "P3039",
          cohort %in% 
            c("P4044", "P4549") ~ "P4049",
          cohort %in% 
            c("P5054", "P5559") ~ "P5059",
          cohort %in% 
            c("P6064", "P6569","P7074", "P7579",
              "P8084", "P8589", "P9094", "P9599",
              "P100up") ~ "P60up"), 
        levels = 
          c("P0029", "P3039", "P4049",
            "P5059", "P60up"))) |>
  # Get also factor 'cohort' to numeric codes
  mutate(cohort_code = as.integer(cohort_short))

```

Let's now create cohorts in our `pp_microsim` data to match our population projection data.

```{r}
# Convert 'age' into 'cohort' factor with levels ordered as specified
pp_microsim10 <- pp_microsim09 |>
    mutate(cohort = factor(case_when(
    age >= 0  & age <= 29 ~ "P0029",
    age >= 30 & age <= 39 ~ "P3039",
    age >= 40 & age <= 49 ~ "P4049",
    age >= 50 & age <= 59 ~ "P5059",
    age >= 60  ~ "P60up"
  ), levels = c("P0029", "P3039", "P4049", "P5059", "P60up")))

# Convert the 'cohort' and 'gender' factor to numeric codes
pp_microsim11 <- pp_microsim10  |> 
  mutate(cohort_code = as.integer(cohort))  |>  
  mutate(gender_code = as.integer(gender)) |> 
  mutate(weight = Weights / 4)

# rm(list = ls(pattern = "^pp_microsim[0-9]+$"))

```

We also need demographic targets for 2030 and 2050

```{r}
#| warning: false
#| message: false

# Ensure pop_targets_2030 is correctly prepared
# We use the "Medium" variant = variants[7]
pop_targets_2030 <- population_projections  |> 
  filter(year == 2030, Variant == variants[7])  |> 
  group_by(cohort_code, cohort_short) |> 
    summarize(female = sum(yf),
              male   = sum(ym), 
              total = sum(total_population),
              ) |>
  ungroup()

pop_targets_2050 <- population_projections  |> 
  filter(year == 2050, Variant == variants[7])  |> 
  group_by(cohort_code, cohort_short) |> 
    summarize(female = sum(yf),
              male   = sum(ym), 
              total = sum(total_population),
              ) |>
  ungroup()

pop_total_2030 <- sum(pop_targets_2030$total)
pop_total_2050 <- sum(pop_targets_2050$total)

pop_targets_2030 <- pop_targets_2030 |> 
  mutate(pct_total = total / pop_total_2030)
pop_targets_2050 <- pop_targets_2050 |> 
  mutate(pct_total = total / pop_total_2050)

#writeClipboard(pop_targets_2030)
# write.table(pop_targets_2030, "clipboard", sep="\t", row.names=FALSE)
  
```

We add economic targets from lmarket, by combining totals from the survey and growth rates by share from the macro file so that shares do not have wild changes (since the shares of lmarket are so different between the macro file and the actual survey). We do this so that labor income doesn't change in a completely radical way.

```{r}
# Survey baseline (e.g. 2023) weighted totals by labor market group
survey_lmarket_base <- pp_microsim11 |>
  group_by(lmarket) |>
  summarize(survey_total = sum(weight, na.rm = TRUE)) |>
  arrange(lmarket)  # Make sure it's in order 1 to 5

survey_totals <- survey_lmarket_base$survey_total  # length 5

# Get the macro growth rates and apply them:
lmarket_targets <- macro_data |>
  filter(year %in% analysis_years, scenario_id %in% scenarios, Variant == variants[7]) |>
  select(scenario_id, year, starts_with("lmarket_")) |>
  # Select only *_growth columns
  select(scenario_id, year, matches("lmarket_\\d+_growth")) |>
  arrange(scenario_id, year) |>
  rowwise() |>
  mutate(
    # Apply growth rate to survey baseline
    l1 = survey_totals[1] * lmarket_1_growth,
    l2 = survey_totals[2] * lmarket_2_growth,
    l3 = survey_totals[3] * lmarket_3_growth,
    l4 = survey_totals[4] * lmarket_4_growth,
    l5 = survey_totals[5] * lmarket_5_growth
  ) |>
  mutate(
    total = l1 + l2 + l3 + l4 + l5,
    target_lmarket_1 = l1 / total,
    target_lmarket_2 = l2 / total,
    target_lmarket_3 = l3 / total,
    target_lmarket_4 = l4 / total,
    target_lmarket_5 = l5 / total
  ) |>
  select(scenario_id, year, starts_with("target_lmarket")) |>
  ungroup()

```


## Reweigting

We use anesrake to calculate targets from known future proportions of sex, age, economic sector. We first create a target list.

```{r}
# Create a list to store targets for each scenario and year
all_targets <- list()

for (sc in scenarios) {
  for (yr in analysis_years) {
    
    # 1. Get population targets
    pop_targets <- population_projections |>
      filter(year == yr, Variant == "Medium") |>
      group_by(cohort_code, cohort_short) |>
      summarize(female = sum(yf),
                male   = sum(ym),
                total  = sum(total_population),
                .groups = "drop") |>
      mutate(pct_total = total / sum(total))
    
    gender_code <- c(
      sum(pop_targets$female) / sum(pop_targets$total),
      sum(pop_targets$male)   / sum(pop_targets$total)
    )
    
    cohort_code <- pop_targets$pct_total

    # 2. Get labor market targets from the newly computed table
    lmarket_code <- lmarket_targets |>
      filter(scenario_id == sc, year == yr) |>
      select(starts_with("target_lmarket")) |>
      unlist(use.names = FALSE)

    # 3. Store targets in named list
    targets <- list(
      gender_code = gender_code,
      cohort_code = cohort_code,
      lmarket = lmarket_code
    )
    names(targets) <- c("gender_code", "cohort_code", "lmarket")

    # 4. Save using naming convention "name_year_scenario"
    key <- paste0("targets_", yr, "_", sc)
    all_targets[[key]] <- targets
  }
}
```

And now we perform the reweighting, using the original weights. Initially we had used the default option type = "pctlim" combined with pctlim=0.05, because the method recommends that if reweighting changes for one variable according to its target are not of at least 5%, then it's not worth burdening the procedure with it. It then ignored sex as a reweighting variable, leaving a small percentage difference between the target and the final population. However, we then tried removing this limitation and the procedure reached convergence in 40 and 49 iterations very efficiently for 2030 and 2050, respectively.

This has now been put into a for loop to facilitate multiple scenarios. See the original Armenia code for a simpler "by hand" version.

```{r}
#| warning: false
#| message: false

rakedata_base <- as.data.frame(pp_microsim11)
original_weight_sum <- sum(rakedata_base$weight)

for (key in names(all_targets)) {
  
  parts <- strsplit(key, "_")[[1]]
  yr <- parts[2]
  sc <- parts[3]
  
  targets <- all_targets[[key]]
  
  cat("🔁 Raking:", yr, sc, "\n")
  
  outsave <- anesrake::anesrake(
    targets,            # <- positional
    rakedata_base,      # <- positional
    caseid       = rakedata_base$MemberId,
    choosemethod = "total",
    type         = "nolim",
    nlim         = 3,
    iterate      = TRUE,
    force1       = TRUE,
    verbose      = TRUE
  )
  
  weight_name <- paste0("weight_", yr, "_", sc)
  new_weight <- unlist(outsave[[1]])
  rakedata_base[[weight_name]] <- new_weight
  
  scaling_factor <- pop_data$total_population[pop_data$year == as.integer(yr)] /
                    pop_data$total_population[pop_data$year == 2023]
  target_sum <- original_weight_sum * scaling_factor
  new_sum <- sum(new_weight)
  rakedata_base[[weight_name]] <- rakedata_base[[weight_name]] * (target_sum / new_sum)
  
  cat("✅ Finished:", weight_name, 
      "| Rescaled sum =", round(sum(rakedata_base[[weight_name]])), "\n\n")
}

```

Weights for the household database. In any household survey, family members share the same household weight. Multiplying that weight by the number of household members over all observations adds to the total population of that year. In the persons dataset, the individual's weight is the household weight divided by the number of family members. But when we do our raking procedure to reweight the dataset to match projected future conditions, we change the individual weights of some family members, but not of others who are not subject to the new conditions. So the family weights need to change to reflect that. This is what is happening below where we create the variables `hh_weight_year_scenario`.

```{r}
# Compute hhsize (if not done already)
hh_size <- rakedata_base |> 
  select(UID, hhsize) |> 
  mutate(ones = 1) |> 
  group_by(UID) |> 
  summarize(hhsize = sum(ones, na.rm = TRUE), .groups = "drop")

# Merge household size back into rakedata_base

rakedata_base <- rakedata_base |>
  rename(hhsize_old = hhsize) |> 
  left_join(hh_size, by = "UID")

# Calculate all hh_weight_* columns
# Get all dynamically created person-weight columns
weight_cols <- names(rakedata_base)[grepl("^weight_\\d{4}_.+", names(rakedata_base))]

# Create household weight columns
for (wcol in weight_cols) {
  hhcol <- sub("weight_", "hh_weight_", wcol)
  rakedata_base[[hhcol]] <- rakedata_base[[wcol]] / rakedata_base$hhsize
}

# Aggregate to household level
# All household-weight columns including 2023
hh_weight_cols <- grep("^hh_weight_", names(rakedata_base), value = TRUE)

weights_scenarios <- rakedata_base |> 
  group_by(UID) |>
  summarize(across(all_of(hh_weight_cols), ~sum(.x, na.rm = TRUE))) |>
  ungroup()

# Save pp_microsim12
pp_microsim12 <- tibble(rakedata_base)

```


# Rescaling labor income according to changes to the wage bill

As a last step, we rescale labor income according to changes to the wage bill in the macro scenario. Recall that in our macrodata file, we estimated growth rates for a number of variables, including the wage bill for the three sectors. We take stock of the wage bill for each of those in the survey year and then we scale each category's total to match the growth rate in the macro file and see how the individual wages change for each individual involved and add back the family labor income to see the impact on total income.

```{r}
# Get wage growth rates for all scenarios/years
wage_growth_factors <- macro_data |>
  filter(year %in% analysis_years, scenario_id %in% scenarios, Variant == variants[7]) |>
  select(
    scenario_id, year,
    rwage_agr_b_lcu_growth,
    rwage_man_b_lcu_growth,
    rwage_ser_b_lcu_growth
  )
```

Note the use of the newer `cross_join()` below, which replaced a left_join(by = character() ) that has been depracated after dplyr 1.1.0.

```{r}
# Compute actual wages by sector and year-scenario weight
wage_results <- list()

for (sc in scenarios) {
  for (yr in analysis_years) {
    wname <- paste0("weight_", yr, "_", sc)

    wages_sector <- pp_microsim12 |>
      filter(!is.na(SAM3_job1)) |>
      group_by(SAM3_job1) |>
      summarize(
        wages_2023 = sum(annual_labor_total * weight, na.rm = TRUE),
        wages_proj = sum(annual_labor_total * .data[[wname]], na.rm = TRUE),
        .groups = "drop"
      ) |>
      cross_join(
        wage_growth_factors |> 
          filter(year == yr, scenario_id == sc)
        ) |>
      mutate(
        wages_target = case_when(
          SAM3_job1 == "Agriculture" ~ wages_2023 * rwage_agr_b_lcu_growth,
          SAM3_job1 == "Manufactures" ~ wages_2023 * rwage_man_b_lcu_growth,
          SAM3_job1 == "Services" ~ wages_2023 * rwage_ser_b_lcu_growth
        ),
        wage_coef = wages_target / wages_proj,
        scenario_id = sc,
        year = yr
      )

    key <- paste0(yr, "_", sc)
    wage_results[[key]] <- wages_sector
  }
}
```

Loop to apply wage_coef to each year-scenario

```{r}
pp_microsim13 <- pp_microsim12 |> 
  rename(
    monthly_labor_income_2023 = monthly_labor_total,
    annual_labor_total_2023   = annual_labor_total
  )

for (sc in scenarios) {
  for (yr in analysis_years) {

    key <- paste0(yr, "_", sc)
    coef_table <- wage_results[[key]]

    # Create a named vector: wage_coef by SAM3_job1
    coef_lookup <- setNames(coef_table$wage_coef, coef_table$SAM3_job1)

    # Create dynamic column names
    mon_out <- paste0("monthly_labor_income_", key)
    ann_out <- paste0("annual_labor_total_", key)

    # Match and apply
    pp_microsim13[[mon_out]] <- coef_lookup[pp_microsim13$SAM3_job1] *
      pp_microsim13$monthly_labor_income_2023

    pp_microsim13[[ann_out]] <- coef_lookup[pp_microsim13$SAM3_job1] *
      pp_microsim13$annual_labor_total_2023
  }
}
```

# Microsimulations

We now implement different shocks according to various scenarios.

## Macro scenarios without additional impacts

First we only adjust labor income according to the reweighting procedure and rescaling of the wage bill. We create household-level total monthly labor income (mli_*) and multiplier coefficients (mli_coef_*) for each scenario. These will allow us to rescale, labor income, and self-employment income. There is a mix of naming conventions within the transformation below born out of mixing different codebases, but the end result is consistent with the naming convention that we have been using here.

Total income in the household dataset (hh_income).

```{r}
# Identify the new income columns
mli_cols <- grep("^monthly_labor_income_\\d{4}_.+", names(pp_microsim13), value = TRUE)

# Household aggregation
hh_li <- pp_microsim13 |>
  group_by(UID) |>
  summarize(
    mli_2023 = sum(monthly_labor_income_2023, na.rm = TRUE),
    across(all_of(mli_cols), ~sum(.x, na.rm = TRUE), .names = "mli_{.col}")
  ) |>
  ungroup()

# Compute multipliers
# Extract only the rescaled mli columns
mli_scen_cols <- grep("^mli_monthly_labor_income_", names(hh_li), value = TRUE)

# Create coefficients
for (col in mli_scen_cols) {
  coef_col <- gsub("mli_monthly_labor_income_", "mli_coef_", col)
  hh_li[[coef_col]] <- ifelse(
    hh_li$mli_2023 == 0, 1,
    hh_li[[col]] / hh_li$mli_2023
  )
}

# Clean final format
# Optional: rename scenario mli columns to match old style (mli_2030_baseline)
names(hh_li) <- gsub("mli_monthly_labor_income_", "mli_", names(hh_li))

# Keep what we need
hh_li <- hh_li |>
  select(UID, starts_with("mli_"), starts_with("mli_coef_"))


```

We are now ready to merge this into hh_income and use the mli_coef_* variables to rescale household-level income per scenario.

Build IC microsim 01

```{r}
ic_microsim01 <- hh_income |>
  left_join(hh_li, by = "UID") |>
  left_join(weights_scenarios, by = "UID") |>
  rename(
    labor_income_2023 = ShemDaq,
    self_employment_income_2023 = ShemTviTdasaqm,
    agr_income_2023 = Shem_Sof,
    totalinc_2023 = Shemosavalisul
  )
```

Rescale incomes using dynamic multipliers.

```{r}
# Find all mli_coef_* columns
coef_cols <- grep("^mli_coef_", names(ic_microsim01), value = TRUE)

# Create rescaled income columns
for (coef in coef_cols) {
  suffix <- sub("mli_coef_", "", coef)

  # Create income variable names
  ic_microsim01[[paste0("labor_income_", suffix)]] <-
    ic_microsim01$labor_income_2023 * ic_microsim01[[coef]]

  ic_microsim01[[paste0("self_employment_income_", suffix)]] <-
    ic_microsim01$self_employment_income_2023 * ic_microsim01[[coef]]
}

```

And recalculate total income.

```{r}
# Find all suffixes again
suffixes <- sub("mli_coef_", "", coef_cols)

for (sfx in suffixes) {
  ic_microsim01[[paste0("totalinc_", sfx)]] <-
    ic_microsim01$totalinc_2023 -
    coalesce(ic_microsim01$labor_income_2023, 0) -
    coalesce(ic_microsim01$self_employment_income_2023, 0) +
    coalesce(ic_microsim01[[paste0("labor_income_", sfx)]], 0) +
    coalesce(ic_microsim01[[paste0("self_employment_income_", sfx)]], 0)
}

```

Recalculate adult-equivalent consumption and poverty status

```{r}
totinc_cols <- grep("^totalinc_", names(ic_microsim01), value = TRUE)
totinc_suffixes <- sub("totalinc_", "", totinc_cols)

for (col in totinc_cols) {
  suffix <- sub("totalinc_", "", col)
  coef_name <- paste0("totinc_coef_", suffix)
  ic_microsim01[[coef_name]] <- ifelse(
    ic_microsim01$totalinc_2023 == 0,
    1,
    ic_microsim01[[col]] / ic_microsim01$totalinc_2023
  )
}

```

Build IC Coef scenarios

```{r}
coef_cols <- grep("^totinc_coef_", names(ic_microsim01), value = TRUE)

ic_coef_scenarios <- ic_microsim01 |>
  select(UID, all_of(coef_cols))

```

And we apply to consumption:

```{r}
ca_microsim01 <- poverty |> 
  left_join(weights_scenarios, join_by(UID == UID)) |> 
  left_join(ic_coef_scenarios, join_by(UID == UID)) |> 
  rename(tc_2023 = totcons) |> 
  mutate(
    hh_weight_2023 = weights,
    adult_equivalent = tc_2023 / aecons  # derives equivalent household size
  )

```

Adjust tc_* and recalculate poverty

```{r}
for (coef in coef_cols) {
  suffix <- sub("totinc_coef_", "", coef)

  tc_col <- paste0("tc_", suffix)
  ae_col <- paste0("aecons_", suffix)
  poor_col <- paste0("poor_", suffix)

  ca_microsim01[[tc_col]] <-
    ca_microsim01$tc_2023 * ca_microsim01[[coef]]

  ca_microsim01[[ae_col]] <-
    ca_microsim01[[tc_col]] / ca_microsim01$adult_equivalent

  ca_microsim01[[poor_col]] <-
    if_else(ca_microsim01[[ae_col]] < pline, 1, 0)
}


```

At this point we re-calculate decile groups for the NZS scenario, to be able to "give back" energy tax revenues to the lower deciles. This scenario is not ready yet, so we need to come back to this. Review Armenia's microsimulation file for this.

## Climate change

In these sections we use Administrative Level 1 data on yield losses and labor productivity losses due to climate change that are provided in the study commissioned for Armenia's CCDR *Estimating the Economic Impacts of Climate Change in Armenia* [@strzepek_estimating_2024].

In the climate change scenario, we ask ourselves, what would happen if agriculture revenues from crops and livestock are reduced due to losses in productivity due to heat? For this, we use crops data.

We add a moving window average and max value for our labor productivity data.

```{r}
# First calculate moving window average
labor_productivity01 <- labor_productivity |>
  # Fix region names
  mutate(
    region = case_when(
      region ==
        "Ajaria" ~ "Adjara A.R.",
      region ==
        "Racha-Lechkhumi-Kvemo Svaneti" ~ "Racha-Lechkhumi and Kvemo Svaneti",
      region ==
        "Abkhazia" ~ "Autonomous Republic of Abkhazia",
      .default = region
    )
  ) |>
  group_by(region,
           scenario) |>
  arrange(year) |>
  # Moving window average 5 years before, 5 after
  mutate(
    moving_avg = rollapply(
      pct_change,
      width = 11,
      FUN = mean,
      partial = TRUE,
      align = "center",
      fill = NA,
      na.rm = TRUE
    )
  ) |>
  ungroup()

# Clim scenarios to select
cs <- sort(unique(labor_productivity01$scenario))

# Moving average for year of interest
# We are interested in cs[1] = "Hot/Dry"
labor_pdcvty_loss <- labor_productivity01 |>
  left_join(
    regions[,c(1:2)],
    join_by(region)
  ) |>
  filter(scenario == cs[1] & year %in% analysis_years) |>
  select(-pct_change,
         -scenario) |>
  pivot_wider(
    names_from = c(sector,year),
    values_from = moving_avg,
    names_prefix = "labprod_")
```

We add a moving window average and max value for our crops productivity data.

```{r}
agriculture_productivity01 <- agriculture_productivity |> 
  # Fix region names
  mutate(
    region = case_when(
      region == 
        "Ajaria" ~ "Adjara A.R.",
      region == 
        "Racha-Lechkhumi-Kvemo Svaneti" ~ "Racha-Lechkhumi and Kvemo Svaneti",
      region ==
        "Abkhazia" ~ "Autonomous Republic of Abkhazia",
      .default = region
    )
  ) |> 
  group_by(region, 
           scenario) |>
  arrange(year) |>
  # Fix missing
  mutate(
    pct_change = if_else(
      is.na(pct_change),0,pct_change
    )
  ) |> 
  # Moving window average
  mutate(
    moving_avg = rollapply(
      pct_change,
      width = 11,
      # 5 years before, 5 after + reference year = 11
      FUN = mean,
      partial = TRUE,
      align = "center",
      fill = NA,
      na.rm = TRUE
    )
  ) |>
  ungroup()

# Clim scenarios to select
cs <- sort(unique(agriculture_productivity01$scenario))

# Moving average for year of interest
ag_pdcvty_loss <- agriculture_productivity01 |>
  left_join(
    regions[,c(1:2)],
    join_by(region)
  ) |> 
  filter(scenario == cs[1] & 
         year %in% analysis_years) |>
  select(-pct_change, 
         -scenario,
         -product) |>
  pivot_wider(
    names_from = c(year), 
    values_from = moving_avg,
    names_prefix = "agprod_")
```

There is no explicit livestock income in this dataset, so we leave livestock  productivity out.

And then we introduce these values in our ag income and labor income data. First, we attach the percentage losses to the appropriate data set.

```{r}
# Persons processed dataset
pp_microsim_cc01 <- pp_microsim13 |>
  left_join(labor_pdcvty_loss, 
            join_by(region == RegNo))

# Household income processed dataset
# We also impact ag income with ag labor productivity
ic_microsim_cc01 <- ic_microsim01 |>
  left_join(hh_basics, join_by(UID)) |> 
  left_join(labor_pdcvty_loss[,c(2,3,6)],
            join_by(RegNo)) |> 
  cross_join(ag_pdcvty_loss[,c(3,4)])

##write.table(lab_loss_avg, "clipboard", sep="\t", row.names=FALSE)
```

And we first shock labor income (need to automate this in a loop).

```{r}
# Labor income according to sector
pp_microsim_cc02 <- pp_microsim_cc01 |>
  mutate(sector = as.numeric(SAM3_job1)) |>
  mutate(
    mli_2030_baseline_cc =
      case_when(
        sector == 1 ~
          monthly_labor_income_2030_baseline * 
          (100 + labprod_Agriculture_2030)/100,
        sector == 2 ~
          monthly_labor_income_2030_baseline * 
          (100 + labprod_Industry_2030)/100,
        sector == 3 ~
          monthly_labor_income_2030_baseline * 
          (100 + labprod_Services_2030)/100,
        TRUE ~ NA
      )
  ) |>
  mutate(
    mli_2050_baseline_cc =
      case_when(
        sector == 1 ~
          monthly_labor_income_2050_baseline * 
          (100 + labprod_Agriculture_2050)/100,
        sector == 2 ~
          monthly_labor_income_2050_baseline * 
          (100 + labprod_Industry_2050)/100,
        sector == 3 ~
          monthly_labor_income_2050_baseline * 
          (100 + labprod_Services_2050)/100,
        TRUE ~ NA
      )
  )
```

We aggregate at household level and take note of the percent difference between the two labor incomes, so that we can impact labor income by that amount. We don't do it with absolute numbers because we don't know the assumptions made by the poverty team to construct the income variable.

```{r}
ic_new_incomes01 <- pp_microsim_cc02 |>
  group_by(UID) |>
  summarize(
    mli_2030_baseline_cc = 
      sum(mli_2030_baseline_cc, na.rm = TRUE),
    mli_2050_baseline_cc = 
      sum(mli_2050_baseline_cc, na.rm = TRUE),
    mli_2030_original = 
      sum(monthly_labor_income_2030_baseline, na.rm = TRUE),
    mli_2050_original = 
      sum(monthly_labor_income_2050_baseline, na.rm = TRUE)
  ) |>
  mutate(
    mli_2030_baseline_cc_coef =
      if_else(
        mli_2030_original == 0 | is.na(mli_2030_original),
        1,
        mli_2030_baseline_cc / mli_2030_original
      ),
    mli_2050_baseline_cc_coef =
      if_else(
        mli_2050_original == 0 | is.na(mli_2050_original),
        1,
        mli_2050_baseline_cc / mli_2050_original
      )
  ) |>
  ungroup()

ic_microsim_cc02 <- ic_microsim_cc01 |>
  left_join(ic_new_incomes01, 
            join_by(UID)) |>
  mutate(
    labor_income_2030_baseline_cc = 
      labor_income_2030_baseline * mli_2030_baseline_cc_coef,
    self_employment_income_2030_baseline_cc = 
      self_employment_income_2030_baseline * mli_2030_baseline_cc_coef,
    labor_income_2050_baseline_cc = 
      labor_income_2050_baseline * mli_2050_baseline_cc_coef,
    self_employment_income_2050_baseline_cc = 
      self_employment_income_2050_baseline * mli_2050_baseline_cc_coef
  )
```

And now we impact agricultural income `agr_income_2030_baseline` (there is no livestock income`lvstk` in this dataset).

And recalculate total income. First labor productivity alone. I learned about pick with this one, because the (.) is no longer interpreted as "this".

```{r}
ic_microsim_cc03 <- ic_microsim_cc02 |>
  # Labor productivity applied to ag income
  mutate(
    agr_income_2030_cc =
      agr_income_2023 *
      # Labor shock
      (100 + labprod_Agriculture_2030)/100 *
      # Crops shock
      (100 + agprod_2030)/100,
    agr_income_2050_cc =
      agr_income_2023 *
      # Labor shock
      (100 + labprod_Agriculture_2050)/100 *
      # Crops shock
      (100 + agprod_2050)/100) |>
  mutate(
    totalinc_2030_baseline_cc =
      totalinc_2030_baseline -
      rowSums(pick(labor_income_2030_baseline,
                   self_employment_income_2030_baseline,
                   agr_income_2023), na.rm = TRUE)
    +
      rowSums(pick(labor_income_2030_baseline_cc,
                   self_employment_income_2030_baseline_cc,
                   agr_income_2030_cc), na.rm = TRUE),

    totalinc_2050_baseline_cc =
      totalinc_2050_baseline -
      rowSums(pick(labor_income_2050_baseline,
                   self_employment_income_2050_baseline,
                   agr_income_2023), na.rm = TRUE) +
      rowSums(pick(labor_income_2050_baseline_cc,
                   self_employment_income_2050_baseline_cc,
                   agr_income_2050_cc), na.rm = TRUE)
  ) |>
  mutate(
    totalinc_2030_baseline_cc_coef =
      if_else(totalinc_2030_baseline == 0,
              1, totalinc_2030_baseline_cc /
                totalinc_2030_baseline),
    totalinc_2050_baseline_cc_coef =
      if_else(totalinc_2050_baseline == 0,
              1, totalinc_2050_baseline_cc /
                totalinc_2050_baseline)
  ) |>
  mutate(
    totalinc_2030_baseline_cc_coef =
      if_else(is.na(totalinc_2030_baseline_cc_coef),
              1, totalinc_2030_baseline_cc_coef),
    totalinc_2050_baseline_cc_coef =
      if_else(is.na(totalinc_2050_baseline_cc_coef),
              1, totalinc_2050_baseline_cc_coef)
  )
```

We assume that the loss in income translates into a loss of expenditure.

```{r}
#| warning: false
#| message: false

income_losses <- ic_microsim_cc03 |> 
  select(UID,
         totalinc_2030_baseline_cc, 
         totalinc_2050_baseline_cc,
         totalinc_2030_baseline_cc_coef, 
         totalinc_2050_baseline_cc_coef)

ca_microsim02 <- ca_microsim01 |> 
  left_join(income_losses, join_by(UID))

# And now reduce total consumption

ca_microsim03 <- ca_microsim02 |> 
  mutate(tc_2030_baseline_cc = tc_2030_baseline *
           totalinc_2030_baseline_cc_coef,
         tc_2050_baseline_cc = tc_2050_baseline *
           totalinc_2050_baseline_cc_coef         
         ) |> 
  mutate(aecons_2030_baseline_cc = 
           tc_2030_baseline_cc / adult_equivalent,
         aecons_2050_baseline_cc = 
           tc_2050_baseline_cc / adult_equivalent) |> 
  mutate(poor_2030_baseline_cc = 
           if_else(aecons_2030_baseline_cc < pline, 1, 0),
         poor_2050_baseline_cc = 
           if_else(aecons_2050_baseline_cc < pline, 1, 0)
         )

##write.table(test, "clipboard", sep="\t", row.names=FALSE)

```

# Agregate consumption effects

We first estimate quintiles from our scenario estimated consumption.

```{r}
ca_microsim04 <- ca_microsim03

# Assuming you already have `analysis_years` and `scenarios` defined

for (yr in analysis_years) {
  for (sc in scenarios) {
    
    # Construct variable names
    inc_var <- paste0("tc_", yr, "_", sc)
    wgt_var <- paste0("hh_weight_", yr, "_", sc)
    qnt_var <- paste0("quintiles_", yr, "_", sc)

    # Extract income and weights
    inc <- ca_microsim04[[inc_var]]
    wgt <- ca_microsim04[[wgt_var]]

    # Replace NAs in income with zero
    inc[is.na(inc)] <- 0

    # Compute weighted quintile cutoffs
    cuts <- Hmisc::wtd.quantile(inc, weights = wgt, probs = seq(0.2, 0.8, 0.2))

    # Cut into quintile groups and assign to new column
    ca_microsim04[[qnt_var]] <- cut(inc,
                                    breaks = c(-Inf, cuts, Inf),
                                    labels = 1:5,
                                    include.lowest = TRUE)
  }
}

```

And now we process the HH consumption effects from the macro team and adjust our dataset accordingly by scenario.

```{r}
# Modify in place: parse quintile and type
hh_consumption_effects <- hh_consumption_effects %>%
  mutate(
    quintile = as.integer(sub("h-hh([1-5])_.*", "\\1", hh_type)),
    type = if_else(grepl("_rur$", hh_type), 2L, 1L)
  ) %>%
  select(-hh_type) %>%
  pivot_longer(
    cols = -c(scenario_id, quintile, type),
    names_to = "year",
    values_to = "shock"
  ) %>%
  mutate(year = as.integer(year))
```

```{r}
# Only use subset of scenarios present in hh_consumption_effects
sc_subset <- unique(hh_consumption_effects$scenario_id)
ca_microsim05 <- ca_microsim04

for (yr in analysis_years) {
  for (sc in sc_subset) {
    
    ae_var <- paste0("aecons_", yr, "_", sc)
    quint_var <- paste0("quintiles_", yr, "_", sc)
    new_var <- paste0("aecons_", yr, "_", sc, "_pc")
    
    shock_df <- hh_consumption_effects %>%
      filter(year == yr, scenario_id == sc) %>%
      select(quintile, type, shock)

    ca_microsim05 <- ca_microsim05 %>%
      mutate(.quintile_join = as.integer(.data[[quint_var]])) %>%
      left_join(shock_df, by = c(".quintile_join" = "quintile", "type" = "type")) %>%
      mutate(!!new_var := .data[[ae_var]] * (1 + shock)) %>%
      select(-shock, -.quintile_join)
  }
}
```

And now we reestimate poverty for these columns.

```{r}
ca_microsim06 <- ca_microsim05
for (yr in analysis_years) {
  for (sc in sc_subset) {
    
    ae_pc_col <- paste0("aecons_", yr, "_", sc, "_pc")
    poor_pc_col <- paste0("poor_", yr, "_", sc, "_pc")
    
    ca_microsim06[[poor_pc_col]] <- if_else(
      ca_microsim06[[ae_pc_col]] < pline, 1L, 0L
    )
  }
}

```


# Poverty tabulates

```{r}
# Initial year
poor_2023 <- ca_microsim06 |>
  group_by(poor_2023) |> 
  summarize(
    no_hh_2023 = sum(hh_weight_2023, na.rm = TRUE),
    no_pp_2023 = sum(hh_weight_2023 * hhsize, na.rm = TRUE)
    ) |> 
  ungroup()

# Baseline
poor_2030_baseline <- ca_microsim06 |>
  group_by(poor_2030_baseline) |> 
  summarize(
    no_hh_2030_baseline = sum(hh_weight_2030_baseline, na.rm = TRUE),
    no_pp_2030_baseline = sum(hh_weight_2030_baseline * hhsize, na.rm = TRUE),
    ) |> 
  ungroup()

poor_2050_baseline <- ca_microsim06 |>
  group_by(poor_2050_baseline) |> 
  summarize(
    no_hh_2050_baseline = sum(hh_weight_2050_baseline, na.rm = TRUE),
    no_pp_2050_baseline = sum(hh_weight_2050_baseline * hhsize, na.rm = TRUE),
    ) |> 
  ungroup()

# Dry/Hot
poor_2030_dryhot <- ca_microsim06 |>
  group_by(poor_2030_dryhot) |> 
  summarize(
    no_hh_2030_dryhot = sum(hh_weight_2030_dryhot, na.rm = TRUE),
    no_pp_2030_dryhot = sum(hh_weight_2030_dryhot * hhsize, na.rm = TRUE),
    ) |> 
  ungroup()

poor_2050_dryhot <- ca_microsim06 |>
  group_by(poor_2050_dryhot) |> 
  summarize(
    no_hh_2050_dryhot = sum(hh_weight_2050_dryhot, na.rm = TRUE),
    no_pp_2050_dryhot = sum(hh_weight_2050_dryhot * hhsize, na.rm = TRUE),
    ) |> 
  ungroup()

# Wet/Warm
poor_2030_wetwarm <- ca_microsim06 |>
  group_by(poor_2030_wetwarm) |> 
  summarize(
    no_hh_2030_wetwarm = sum(hh_weight_2030_wetwarm, na.rm = TRUE),
    no_pp_2030_wetwarm = sum(hh_weight_2030_wetwarm * hhsize, na.rm = TRUE),
    ) |> 
  ungroup()

poor_2050_wetwarm <- ca_microsim06 |>
  group_by(poor_2050_wetwarm) |> 
  summarize(
    no_hh_2050_wetwarm = sum(hh_weight_2050_wetwarm, na.rm = TRUE),
    no_pp_2050_wetwarm = sum(hh_weight_2050_wetwarm * hhsize, na.rm = TRUE),
    ) |> 
  ungroup()

# NZS
poor_2030_nzs <- ca_microsim06 |>
  group_by(poor_2030_nzs) |> 
  summarize(
    no_hh_2030_nzs = sum(hh_weight_2030_nzs, na.rm = TRUE),
    no_pp_2030_nzs = sum(hh_weight_2030_nzs * hhsize, na.rm = TRUE),
    ) |> 
  ungroup()

poor_2050_nzs <- ca_microsim06 |>
  group_by(poor_2050_nzs) |> 
  summarize(
    no_hh_2050_nzs = sum(hh_weight_2050_nzs, na.rm = TRUE),
    no_pp_2050_nzs = sum(hh_weight_2050_nzs * hhsize, na.rm = TRUE),
    ) |> 
  ungroup()

poor_2030_baseline_cc <- ca_microsim06 |>
  group_by(poor_2030_baseline_cc) |>
  summarize(
    no_hh_2030_baseline_cc = sum(hh_weight_2030_baseline, na.rm = TRUE),
    no_pp_2030_baseline_cc = sum(hh_weight_2030_baseline * hhsize, na.rm = TRUE),
    ) |>
  ungroup()

poor_2050_baseline_cc <- ca_microsim06 |>
  group_by(poor_2050_baseline_cc) |>
  summarize(
    no_hh_2050_baseline_cc = sum(hh_weight_2050_baseline, na.rm = TRUE),
    no_pp_2050_baseline_cc = sum(hh_weight_2050_baseline * hhsize, na.rm = TRUE),
    ) |>
  ungroup()

# Price effects

# Dry/Hot
poor_2030_dryhot_pc <- ca_microsim06 |>
  group_by(poor_2030_dryhot_pc) |> 
  summarize(
    no_hh_2030_dryhot = sum(hh_weight_2030_dryhot, na.rm = TRUE),
    no_pp_2030_dryhot = sum(hh_weight_2030_dryhot * hhsize, na.rm = TRUE),
    ) |> 
  ungroup()

poor_2050_dryhot_pc <- ca_microsim06 |>
  group_by(poor_2050_dryhot_pc) |> 
  summarize(
    no_hh_2050_dryhot = sum(hh_weight_2050_dryhot, na.rm = TRUE),
    no_pp_2050_dryhot = sum(hh_weight_2050_dryhot * hhsize, na.rm = TRUE),
    ) |> 
  ungroup()

# Wet/Warm
poor_2030_wetwarm_pc <- ca_microsim06 |>
  group_by(poor_2030_wetwarm_pc) |> 
  summarize(
    no_hh_2030_wetwarm = sum(hh_weight_2030_wetwarm, na.rm = TRUE),
    no_pp_2030_wetwarm = sum(hh_weight_2030_wetwarm * hhsize, na.rm = TRUE),
    ) |> 
  ungroup()

poor_2050_wetwarm_pc <- ca_microsim06 |>
  group_by(poor_2050_wetwarm_pc) |> 
  summarize(
    no_hh_2050_wetwarm = sum(hh_weight_2050_wetwarm, na.rm = TRUE),
    no_pp_2050_wetwarm = sum(hh_weight_2050_wetwarm * hhsize, na.rm = TRUE),
    ) |> 
  ungroup()

# NZS
poor_2030_nzs_pc <- ca_microsim06 |>
  group_by(poor_2030_nzs_pc) |> 
  summarize(
    no_hh_2030_nzs = sum(hh_weight_2030_nzs, na.rm = TRUE),
    no_pp_2030_nzs = sum(hh_weight_2030_nzs * hhsize, na.rm = TRUE),
    ) |> 
  ungroup()

poor_2050_nzs_pc <- ca_microsim06 |>
  group_by(poor_2050_nzs_pc) |> 
  summarize(
    no_hh_2050_nzs = sum(hh_weight_2050_nzs, na.rm = TRUE),
    no_pp_2050_nzs = sum(hh_weight_2050_nzs * hhsize, na.rm = TRUE),
    ) |> 
  ungroup()

# write.table(poor_2023, file = pipe("xclip -selection clipboard"), sep = "\t", row.names = FALSE)
# write.table(poor_2030_baseline, file = pipe("xclip -selection clipboard"), sep = "\t", row.names = FALSE)
# write.table(poor_2050_baseline, file = pipe("xclip -selection clipboard"), sep = "\t", row.names = FALSE)
# write.table(poor_2030_dryhot, file = pipe("xclip -selection clipboard"), sep = "\t", row.names = FALSE)
# write.table(poor_2050_dryhot, file = pipe("xclip -selection clipboard"), sep = "\t", row.names = FALSE)
# write.table(poor_2030_wetwarm, file = pipe("xclip -selection clipboard"), sep = "\t", row.names = FALSE)
# write.table(poor_2050_wetwarm, file = pipe("xclip -selection clipboard"), sep = "\t", row.names = FALSE)
# write.table(poor_2030_nzs, file = pipe("xclip -selection clipboard"), sep = "\t", row.names = FALSE)
# write.table(poor_2050_nzs, file = pipe("xclip -selection clipboard"), sep = "\t", row.names = FALSE)
# write.table(poor_2030_baseline_cc, file = pipe("xclip -selection clipboard"), sep = "\t", row.names = FALSE)
# write.table(poor_2050_baseline_cc, file = pipe("xclip -selection clipboard"), sep = "\t", row.names = FALSE)
# write.table(poor_2030_dryhot_pc, file = pipe("xclip -selection clipboard"), sep = "\t", row.names = FALSE)
# write.table(poor_2050_dryhot_pc, file = pipe("xclip -selection clipboard"), sep = "\t", row.names = FALSE)
# write.table(poor_2030_wetwarm_pc, file = pipe("xclip -selection clipboard"), sep = "\t", row.names = FALSE)
# write.table(poor_2050_wetwarm_pc, file = pipe("xclip -selection clipboard"), sep = "\t", row.names = FALSE)
# write.table(poor_2030_nzs_pc, file = pipe("xclip -selection clipboard"), sep = "\t", row.names = FALSE)
# write.table(poor_2050_nzs_pc, file = pipe("xclip -selection clipboard"), sep = "\t", row.names = FALSE)
```

Scenario drivers.

```{r}
macro_drivers <- macro_data |> filter(year==2030) |> select(c(scenario_id,ends_with("growth")))
# write.table(macro_drivers, file = pipe("xclip -selection clipboard"), sep = "\t", row.names = FALSE)
```



# Inequality measurements

Here we estimate weighted inequality measurements.

```{r}
ca_microsim07 <- ca_microsim06 #|> 
  # filter(RegNo == 8)

# Get list of income and weight variable pairs
income_vars <- names(ca_microsim07)[grepl("^aecons_\\d{4}(_[a-zA-Z0-9]+)?$", names(ca_microsim07))]

# We had to add the subscenarios by hand, because of poor planning in naming conventions
lookup_table <- tibble(
  Income_Variable = c(income_vars, 
                      "aecons_2030_baseline_cc", 
                      "aecons_2050_baseline_cc",
                      "aecons_2030_dryhot_pc", 
                      "aecons_2050_dryhot_pc",
                      "aecons_2030_wetwarm_pc", 
                      "aecons_2050_wetwarm_pc",
                      "aecons_2030_nzs_pc", 
                      "aecons_2050_nzs_pc"),
  Weight_Variable = c(gsub("^aecons_", "hh_weight_", income_vars),
                      "hh_weight_2030_baseline",
                      "hh_weight_2050_baseline",
                      "hh_weight_2030_dryhot",
                      "hh_weight_2050_dryhot",
                      "hh_weight_2030_wetwarm",
                      "hh_weight_2050_wetwarm",
                      "hh_weight_2030_nzs",
                      "hh_weight_2050_nzs")
)

# Weighted FGT function
weighted_FGT <- function(income, weight, pline, alpha) {
  if (alpha == 0) {
    return(sum(weight[income < pline], na.rm = TRUE) / sum(weight, na.rm = TRUE))
  } else {
    gap <- pmax(0, (pline - income) / pline)
    return(sum((gap ^ alpha) * weight, na.rm = TRUE) / sum(weight, na.rm = TRUE))
  }
}

# Measure calculation function
calculate_measures <- function(income_var, weight_var) {
  income <- ca_microsim07[[income_var]]
  weight <- ca_microsim07[[weight_var]] * ca_microsim07$hhsize
  
  data.frame(
    `Income Variable` = income_var,
    `FGT (alpha=0) - Poverty Headcount` = weighted_FGT(income, weight, pline, alpha = 0),
    `FGT (alpha=1) - Poverty Gap`       = weighted_FGT(income, weight, pline, alpha = 1),
    `FGT (alpha=2) - Poverty Severity`  = weighted_FGT(income, weight, pline, alpha = 2),
    `Gini Coefficient` = acid::weighted.gini(income, w = weight)
  )
}


# Run it
inequality_results <- pmap_dfr(
  list(lookup_table$Income_Variable, lookup_table$Weight_Variable),
  calculate_measures) |> 
  select(-ends_with(".Gini"), -ends_with("bcGini"))

# Review results
print(inequality_results)
# write.table(inequality_results, file = pipe("xclip -selection clipboard"), sep = "\t", row.names = FALSE)
```

Plotting inequality among scenarios and years

```{r}
library(dplyr)
library(tidyr)
library(ggplot2)

# Prepare data as before
inequality_results_long <- inequality_results %>%
  mutate(
    year = as.integer(sub("aecons_(\\d{4}).*", "\\1", Income.Variable)),
    scenario = sub("aecons_\\d{4}_?", "", Income.Variable),
    scenario = ifelse(scenario == "", "2023", scenario)
  ) %>%
  filter(year != 2023) %>%
  mutate(
    scenario = factor(scenario),
    year = factor(year),
    Poverty_Headcount = 100 * FGT..alpha.0....Poverty.Headcount,
    Poverty_Gap = 100 * FGT..alpha.1....Poverty.Gap,
    Poverty_Severity = 100 * FGT..alpha.2....Poverty.Severity
  ) %>%
  select(year, scenario, Poverty_Headcount, Poverty_Gap, Poverty_Severity) %>%
  pivot_longer(
    cols = c(Poverty_Headcount, Poverty_Gap, Poverty_Severity),
    names_to = "Measure",
    values_to = "Value"
  ) %>%
  mutate(Measure = dplyr::recode(as.character(Measure),
  "Poverty_Headcount" = "Poverty Headcount",
  "Poverty_Gap" = "Poverty Gap",
  "Poverty_Severity" = "Poverty Severity"
))


# Plot with value labels
ggplot(inequality_results_long, aes(x = scenario, y = Value, fill = scenario)) +
  geom_col(position = position_dodge(width = 0.9), width = 0.8) +
  geom_text(aes(label = sprintf("%.1f", Value)),
          vjust = -0.3, size = 2.5, position = position_dodge(width = 0.9)) +
  facet_grid(Measure ~ year, scales = "free_y") +
  labs(
    title = "Poverty Metrics by Scenario and Year",
    x = "Scenario",
    y = "Value (%)",
    fill = "Scenario"
  ) +
  theme_minimal(base_size = 11) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))


```


Changes by region

```{r}
poor_2030_cc_region <- ca_microsim06 |> 
  group_by(RegNo,poor_2030_baseline_cc) |>  
  summarize(
    # no_hh_2023 = sum(weight_2023, na.rm = TRUE),
    no_pp_2030_cc = sum(hh_weight_2030_baseline * hhsize, na.rm = TRUE)
    ) |> 
  pivot_wider(
    id_cols = RegNo,
    names_from = poor_2030_baseline_cc,
    values_from = c(
      # no_hh_2023,
      no_pp_2030_cc)
  ) |> 
  mutate(
    RegNo = as.integer(RegNo),
    pct_poor = (`1` / (`1` + `0`))*100
  ) |> 
  rename(
    poor = `1`,
    non_poor = `0`
  ) |> 
  ungroup()

poor_2030_baseline_region <- ca_microsim06 |> 
  group_by(RegNo,poor_2030_baseline) |>  
  summarize(
    # no_hh_2023 = sum(weight_2023, na.rm = TRUE),
    no_pp_2030_baseline = sum(hh_weight_2030_baseline * hhsize, na.rm = TRUE)
    ) |> 
  pivot_wider(
    id_cols = RegNo,
    names_from = poor_2030_baseline,
    values_from = c(
      # no_hh_2023,
      no_pp_2030_baseline)
  ) |> 
  mutate(
    RegNo = as.integer(RegNo),
    pct_poor = (`1` / (`1` + `0`))*100
  ) |> 
  rename(
    poor = `1`,
    non_poor = `0`
  ) |> 
  ungroup()

# write.table(poor_2050_baseline, file = pipe("xclip -selection clipboard"), sep = "\t", row.names = FALSE)
```



# Maps

```{r}
#| warning: false
#| message: false
#| label: fig-exposed-and-poor
#| fig-cap: "Percent below poverty line by region"

poor_2023_region <- ca_microsim06  |> 
  group_by(RegNo,poor_2023) |>  
  summarize(
    # no_hh_2023 = sum(weight_2023, na.rm = TRUE),
    no_pp_2023 = sum(hh_weight_2023 * hhsize, na.rm = TRUE)
    ) |> 
  pivot_wider(
    id_cols = RegNo,
    names_from = poor_2023,
    values_from = c(
      # no_hh_2023,
      no_pp_2023)
  ) |> 
  mutate(
    RegNo = as.integer(RegNo),
    pct_poor = (`1` / (`1` + `0`))*100
  ) |> 
  rename(
    poor = `1`,
    non_poor = `0`
  ) |> 
  ungroup()

poor_2023_region_map <- adm1 |> 
  left_join(poor_2023_region, join_by(RegNo)) |> 
  mutate(
    region = case_when(
      region == "Racha-Lechkhumi and Kvemo Svaneti" ~
      "Racha-Lechkhumi\nand Kvemo Svaneti",
      region == "Samegrelo-Zemo Svaneti" ~
      "Samegrelo-\nZemo\nSvaneti",
      region == "Mtskheta-Mtianeti" ~
      "Mtskheta-\nMtianeti",
      region == "Samtskhe-Javakheti" ~
      "Samtskhe-\nJavakheti",
      .default = region
    ),
    pct_poor_label = if_else(
      is.na(pct_poor), 
      "Unavailable",
      paste(
        region,
        sprintf("\n%.1f%%", pct_poor)
      )
    )
  )

map_object <-
tm_shape(poor_2023_region_map) +
  tm_polygons("pct_poor",
              title="Percent", 
              legend.show = TRUE,
              style = "fixed",
              scale = 0.5,
              breaks = c(0, 5, 10, 15, 20, 25, 30, 35),
              palette = "YlOrRd",
              textNA = "Unavailable",
              colorNA = "grey"
              ) +
  tm_text(c("pct_poor_label"), size = .7, col = "black")+
  tm_layout(
    #legend.outside = TRUE,
    legend.position = c("right", "top"),
    #title.snap.to.legend = FALSE,
    title = 
      "Percent below poverty line",
    frame = FALSE,
#            outer.margins=c(.10,.10, .10, .10), 
            title.position = c('left', 'bottom'),
            title.size = 0.9)

# tmap_save(
#   map_object,
#   "data/outputs/vulnerability_img/other/poverty_by_region.svg",
#   width = 8,
#   height = 5,
#   units = "in"
# )
# tmap_save(
#   map_object,
#   "data/outputs/vulnerability_img/other/poverty_by_region.png",
#   width = 8,
#   height = 5,
#   units = "in",
#   dpi = 300
# )
map_object


# write.table(poor_numbers, file = pipe("xclip -selection clipboard"), sep = "\t", row.names = FALSE)
```

Map request: Increase in poverty between Baseline and NZS and between Baseline and Hot/Dry at the regional level for 2030.

```{r}
poor_2030_baseline_region <- ca_microsim06 |> 
  group_by(RegNo,poor_2030_baseline) |>  
  summarize(
    # no_hh_2023 = sum(weight_2023, na.rm = TRUE),
    no_pp_2030_baseline = sum(hh_weight_2030_baseline * hhsize, na.rm = TRUE)
    ) |> 
  pivot_wider(
    id_cols = RegNo,
    names_from = poor_2030_baseline,
    values_from = c(
      # no_hh_2023,
      no_pp_2030_baseline)
  ) |> 
  mutate(
    RegNo = as.integer(RegNo),
    pct_poor = (`1` / (`1` + `0`))*100
  ) |> 
  rename(
    poor_2030_baseline = `1`,
    non_poor = `0`
  ) |> 
  ungroup() |> 
  select(RegNo, poor_2030_baseline)

poor_2030_nzs_region <- ca_microsim06 |> 
  group_by(RegNo,poor_2030_nzs) |>  
  summarize(
    # no_hh_2023 = sum(weight_2023, na.rm = TRUE),
    no_pp_2030_nzs = sum(hh_weight_2030_nzs * hhsize, na.rm = TRUE)
    ) |> 
  pivot_wider(
    id_cols = RegNo,
    names_from = poor_2030_nzs,
    values_from = c(
      # no_hh_2023,
      no_pp_2030_nzs)
  ) |> 
  mutate(
    RegNo = as.integer(RegNo),
    pct_poor = (`1` / (`1` + `0`))*100
  ) |> 
  rename(
    poor_2030_nzs = `1`,
    non_poor = `0`
  ) |> 
  ungroup() |> 
  select(RegNo, poor_2030_nzs)

poor_2030_dryhot_region <- ca_microsim06 |> 
  group_by(RegNo,poor_2030_dryhot) |>  
  summarize(
    # no_hh_2023 = sum(weight_2023, na.rm = TRUE),
    no_pp_2030_dryhot = sum(hh_weight_2030_dryhot * hhsize, na.rm = TRUE)
    ) |> 
  pivot_wider(
    id_cols = RegNo,
    names_from = poor_2030_dryhot,
    values_from = c(
      # no_hh_2023,
      no_pp_2030_dryhot)
  ) |> 
  mutate(
    RegNo = as.integer(RegNo),
    pct_poor = (`1` / (`1` + `0`))*100
  ) |> 
  rename(
    poor_2030_dryhot = `1`,
    non_poor = `0`
  ) |> 
  ungroup() |> 
  select(RegNo, poor_2030_dryhot)

poor_2030_scenarios_diff <- poor_2030_baseline_region |> 
  left_join(poor_2030_nzs_region, join_by(RegNo)) |> 
  left_join(poor_2030_dryhot_region, join_by(RegNo)) |> 
  mutate(
    poor_2030_nzs_diff = (poor_2030_nzs - poor_2030_baseline)/poor_2030_baseline * 100,
    poor_2030_dryhot_diff = (poor_2030_dryhot - poor_2030_baseline) / poor_2030_baseline *100
  )

```

And now we map it.

```{r}
poor_2030_scenarios_diff_map <- adm1 |> 
  left_join(poor_2030_scenarios_diff, join_by(RegNo)) |> 
  mutate(
    region = case_when(
      region == "Racha-Lechkhumi and Kvemo Svaneti" ~
      "Racha-Lechkhumi\nand Kvemo Svaneti",
      region == "Samegrelo-Zemo Svaneti" ~
      "Samegrelo-\nZemo\nSvaneti",
      region == "Mtskheta-Mtianeti" ~
      "Mtskheta-\nMtianeti",
      region == "Samtskhe-Javakheti" ~
      "Samtskhe-\nJavakheti",
      .default = region
    ),
    pct_nzs_label = if_else(
      is.na(poor_2030_nzs_diff), 
      "Unavailable",
      paste(
        region,
        sprintf("\n%.1f%%", poor_2030_nzs_diff)
      )
    ),
    pct_dryhot_label = if_else(
      is.na(poor_2030_dryhot_diff), 
      "Unavailable",
      paste(
        region,
        sprintf("\n%.1f%%", poor_2030_dryhot_diff)
      )
    )
  )

map_object <-
tm_shape(poor_2030_scenarios_diff_map) +
  tm_polygons("poor_2030_nzs_diff",
              title="Percent", 
              legend.show = TRUE,
              style = "fixed",
              scale = 0.5,
              breaks = c(-3, -2, -1, 0, 1,2,3),
              palette = "-RdYlGn",
              textNA = "Unavailable",
              colorNA = "grey"
              ) +
  tm_text(c("pct_nzs_label"), size = .7, col = "black")+
  tm_layout(
    #legend.outside = TRUE,
    legend.position = c("right", "top"),
    #title.snap.to.legend = FALSE,
    title = 
      "Percent difference between NZS and Baseline",
    frame = FALSE,
#            outer.margins=c(.10,.10, .10, .10), 
            title.position = c('left', 'bottom'),
            title.size = 0.9)

# tmap_save(
#   map_object,
#   "data/outputs/vulnerability_img/other/poor_2030_nzs_diff.png",
#   width = 8,
#   height = 5,
#   units = "in",
#   dpi = 300
# )
# tmap_save(
#   map_object,
#   "data/outputs/vulnerability_img/other/poor_2030_nzs_diff.svg",
#   width = 8,
#   height = 5,
#   units = "in"
# )

map_object <-
tm_shape(poor_2030_scenarios_diff_map) +
  tm_polygons("poor_2030_dryhot_diff",
              title="Percent", 
              legend.show = TRUE,
              style = "fixed",
              scale = 0.5,
              breaks = c(-3, -2, -1, 0, 1,2,3),
              palette = "-RdYlGn",
              textNA = "Unavailable",
              colorNA = "grey"
              ) +
  tm_text(c("pct_dryhot_label"), size = .7, col = "black")+
  tm_layout(
    #legend.outside = TRUE,
    legend.position = c("right", "top"),
    #title.snap.to.legend = FALSE,
    title = 
      "Percent difference between Dry/Hot and Baseline",
    frame = FALSE,
#            outer.margins=c(.10,.10, .10, .10), 
            title.position = c('left', 'bottom'),
            title.size = 0.9)

# tmap_save(
#   map_object,
#   "data/outputs/vulnerability_img/other/poor_2030_dryhot_diff.png",
#   width = 8,
#   height = 5,
#   units = "in",
#   dpi = 300
# )
# tmap_save(
#   map_object,
#   "data/outputs/vulnerability_img/other/poor_2030_dryhot_diff.svg",
#   width = 8,
#   height = 5,
#   units = "in"
# )

```

Map request: Increase in poverty between Baseline and NZS + PC and between Baseline and Hot/Dry + PC at the regional level for 2030. Effects of prices.

```{r}
poor_2030_baseline_region <- ca_microsim06 |> 
  group_by(RegNo,poor_2030_baseline) |>  
  summarize(
    # no_hh_2023 = sum(weight_2023, na.rm = TRUE),
    no_pp_2030_baseline = sum(hh_weight_2030_baseline * hhsize, na.rm = TRUE)
    ) |> 
  pivot_wider(
    id_cols = RegNo,
    names_from = poor_2030_baseline,
    values_from = c(
      # no_hh_2023,
      no_pp_2030_baseline)
  ) |> 
  mutate(
    RegNo = as.integer(RegNo),
    pct_poor = (`1` / (`1` + `0`))*100
  ) |> 
  rename(
    poor_2030_baseline = `1`,
    non_poor = `0`
  ) |> 
  ungroup() |> 
  select(RegNo, poor_2030_baseline)

poor_2030_nzs_region <- ca_microsim06 |> 
  group_by(RegNo,poor_2030_nzs_pc) |>  
  summarize(
    # no_hh_2023 = sum(weight_2023, na.rm = TRUE),
    no_pp_2030_nzs = sum(hh_weight_2030_nzs * hhsize, na.rm = TRUE)
    ) |> 
  pivot_wider(
    id_cols = RegNo,
    names_from = poor_2030_nzs_pc,
    values_from = c(
      # no_hh_2023,
      no_pp_2030_nzs)
  ) |> 
  mutate(
    RegNo = as.integer(RegNo),
    pct_poor = (`1` / (`1` + `0`))*100
  ) |> 
  rename(
    poor_2030_nzs = `1`,
    non_poor = `0`
  ) |> 
  ungroup() |> 
  select(RegNo, poor_2030_nzs)

poor_2030_dryhot_region <- ca_microsim06 |> 
  group_by(RegNo,poor_2030_dryhot_pc) |>  
  summarize(
    # no_hh_2023 = sum(weight_2023, na.rm = TRUE),
    no_pp_2030_dryhot = sum(hh_weight_2030_dryhot * hhsize, na.rm = TRUE)
    ) |> 
  pivot_wider(
    id_cols = RegNo,
    names_from = poor_2030_dryhot_pc,
    values_from = c(
      # no_hh_2023,
      no_pp_2030_dryhot)
  ) |> 
  mutate(
    RegNo = as.integer(RegNo),
    pct_poor = (`1` / (`1` + `0`))*100
  ) |> 
  rename(
    poor_2030_dryhot = `1`,
    non_poor = `0`
  ) |> 
  ungroup() |> 
  select(RegNo, poor_2030_dryhot)

poor_2030_scenarios_pc_diff <- poor_2030_baseline_region |> 
  left_join(poor_2030_nzs_region, join_by(RegNo)) |> 
  left_join(poor_2030_dryhot_region, join_by(RegNo)) |> 
  mutate(
    poor_2030_nzs_diff = (poor_2030_nzs - poor_2030_baseline)/poor_2030_baseline * 100,
    poor_2030_dryhot_diff = (poor_2030_dryhot - poor_2030_baseline) / poor_2030_baseline *100
  )

```

And now we map it.

```{r}
poor_2030_scenarios_pc_diff_map <- adm1 |> 
  left_join(poor_2030_scenarios_pc_diff, join_by(RegNo)) |> 
  mutate(
    region = case_when(
      region == "Racha-Lechkhumi and Kvemo Svaneti" ~
      "Racha-Lechkhumi\nand Kvemo Svaneti",
      region == "Samegrelo-Zemo Svaneti" ~
      "Samegrelo-\nZemo\nSvaneti",
      region == "Mtskheta-Mtianeti" ~
      "Mtskheta-\nMtianeti",
      region == "Samtskhe-Javakheti" ~
      "Samtskhe-\nJavakheti",
      .default = region
    ),
    pct_nzs_label = if_else(
      is.na(poor_2030_nzs_diff), 
      "Unavailable",
      paste(
        region,
        sprintf("\n%.1f%%", poor_2030_nzs_diff)
      )
    ),
    pct_dryhot_label = if_else(
      is.na(poor_2030_dryhot_diff), 
      "Unavailable",
      paste(
        region,
        sprintf("\n%.1f%%", poor_2030_dryhot_diff)
      )
    )
  )

map_object <-
tm_shape(poor_2030_scenarios_pc_diff_map) +
  tm_polygons(fill = "poor_2030_nzs_diff",
              fill.scale = tm_scale_intervals(
                style = "fixed",
                midpoint = 0,
                breaks = c(-1,0,1, 2,4,6,8,10,12),
                values = "-brewer.rd_yl_gn",
                value.na = NA,
                label.na = "Unavailable"),
              fill.legend = tm_legend(
                title = "Percent")
              ) +
  tm_title("Percent difference between\nNZS + PC and Baseline") +
  tm_text(c("pct_nzs_label"), size = .7, col = "black")+
  tm_layout(
    #legend.outside = TRUE,
    legend.position = c("right", "top"),
    frame = FALSE,
            title.position = c('left', 'bottom'),
            title.size = 0.9)

# tmap_save(
#   map_object,
#   "data/outputs/vulnerability_img/other/poor_2030_nzs_pc_diff.png",
#   width = 8,
#   height = 5,
#   units = "in",
#   dpi = 300
# )
# tmap_save(
#   map_object,
#   "data/outputs/vulnerability_img/other/poor_2030_nzs_pc_diff.svg",
#   width = 8,
#   height = 5,
#   units = "in"
# )

map_object <-
tm_shape(poor_2030_scenarios_pc_diff_map) +
  tm_polygons(fill = "poor_2030_dryhot_diff",
              fill.scale = tm_scale_intervals(
                style = "fixed",
                midpoint = 0,
                breaks = c(-1, 0, 1,2,4,6,8,10,12),
                values = "-brewer.rd_yl_gn",
                value.na = NA,
                label.na = "Unavailable"),
              fill.legend = tm_legend(
                title = "Percent")
              ) +
  tm_text(c("pct_dryhot_label"), size = .7, col = "black") +
  tm_title("Percent difference between\nDry/Hot + PC and Baseline") +
  tm_layout(
    legend.position = c("right", "top"),
    frame = FALSE,
#           outer.margins=c(.10,.10, .10, .10), 
            title.position = c('left', 'bottom'),
            title.size = 0.9)

# tmap_save(
#   map_object,
#   "data/outputs/vulnerability_img/other/poor_2030_dryhot_pc_diff.png",
#   width = 8,
#   height = 5,
#   units = "in",
#   dpi = 300
# )
# tmap_save(
#   map_object,
#   "data/outputs/vulnerability_img/other/poor_2030_dryhot_pc_diff.svg",
#   width = 8,
#   height = 5,
#   units = "in"
# )

```


Maps show a very interesting behavior in Mtskheta-Mtianeti, where most of the changes in poverty between scenarios concentrate. Here, we will explore a few possible pathways to explain why that is happening.

```{r}
lmarket_region <- pp_microsim13 |> 
  group_by(region, lmarket) |> 
  summarize(
    pp_labor = sum(weight_2030_dryhot, na.rm = T)
  ) |> 
  pivot_wider(
    id_cols = region,
    names_from = lmarket,
    values_from = pp_labor
  ) |> 
  left_join(regions[,c(1,4)], join_by(region == RegNo))

# write.table(lmarket_region, file = pipe("xclip -selection clipboard"), sep = "\t", row.names = FALSE)
```

Employment does not appear to be the culprit as changes between scenarios appear similar for all regions. Now we will explore changes in sources of income.

```{r}
income_regions <- pp_microsim13 |>
  filter(lmarket <= 3) |> 
  group_by(region) |> 
  summarize(
    lab_inc_baseline = sum(monthly_labor_income_2030_baseline * weight_2030_baseline, na.rm = T),
    lab_inc_dryhot = sum(monthly_labor_income_2030_dryhot * weight_2030_dryhot, na.rm = T),
    lab_inc_nzs = sum(monthly_labor_income_2030_nzs * weight_2030_nzs, na.rm = T)
  ) |> 
  left_join(regions[,c(1,4)], join_by(region == RegNo))

# write.table(income_regions, file = pipe("xclip -selection clipboard"), sep = "\t", row.names = FALSE)
```

Income does not give us any particular indications either, so the last thing we can check out is the share that labor income has in this region versus other regions.

```{r}
income_share_regions <- ic_microsim01 |> 
  left_join(hh_basics, join_by(UID)) |>
  mutate(
    weight_2023 = Weights/4,
    shr_lab = (coalesce(labor_income_2023,0.0001) + coalesce(self_employment_income_2023,0)) / totalinc_2023,
    shr_agr = coalesce(agr_income_2023,0.0001)/totalinc_2023,
    shr_ssp = coalesce(PensStipDaxm,0.0001)/totalinc_2023,
    shr_rem = coalesce(Ucxoetidan,0.0001)/totalinc_2023,
    shr_csh = coalesce(Axloblebisagan,0.0001)/totalinc_2023
  ) |> 
  group_by(RegNo) |>
  summarize(
    avg_shr_lab = weighted.mean(shr_lab, weight_2023, na.rm = T),
    avg_shr_agr = weighted.mean(shr_agr, weight_2023, na.rm = T),
    avg_shr_ssp = weighted.mean(shr_ssp, weight_2023, na.rm = T),
    avg_shr_rem = weighted.mean(shr_rem, weight_2023, na.rm = T),
    avg_shr_csh = weighted.mean(shr_csh, weight_2023, na.rm = T)
  ) |> 
  left_join(regions[,c(1,4)], join_by(RegNo == RegNo))

# write.table(income_share_regions, file = pipe("xclip -selection clipboard"), sep = "\t", row.names = FALSE)
```

This exploration did not yield  anything noteworthy. So, now we turn our attention to distance above poverty line.

```{r}
distance_pl <- ca_microsim06 |> 
  filter(aecons_2023 > pline0) |> 
  mutate(
    distance_above_pl = coalesce(aecons_2023,0) - pline0
  ) |> 
  group_by(RegNo) |> 
  summarize(
    avg_distance_above_pl = weighted.mean(distance_above_pl, weights)
  ) |> 
  left_join(regions[,c(1,4)], join_by(RegNo == RegNo))
distance_pl |> 
  gt()

# write.table(distance_pl, file = pipe("xclip -selection clipboard"), sep = "\t", row.names = FALSE)
```

And sure enough, the combination between mean distance above the poverty line, the share of total income that comes from labor, and the share of workers in manufacture explain the regional results quite well.

Now we need to explain why impacts from prices are so relevant and the only mechanism that we used for that was rurality, quintiles, and adult equivalent consumption, so we will explore once again the distance above the poverty line and the changes by region, urban/rural, and quintile.

```{r}
distance_pl <- ca_microsim06 |> 
  # filter(RegNo==8) |> 
  mutate(
    distance_above_pl = if_else(
      aecons_2030_baseline > pline0,
      coalesce(aecons_2030_baseline,0) - pline0,
      0),
    no_pp = hh_weight_2030_baseline * hhsize
  ) |> 
  group_by(type, quintiles_2030_baseline) |> 
  summarize(
    avg_distance_above_pl = weighted.mean(distance_above_pl, hh_weight_2030_baseline),
    no_pp = sum(no_pp, na.rm = T)
  ) 
distance_pl |> 
  gt()
# write.table(distance_pl, file = pipe("xclip -selection clipboard"), sep = "\t", row.names = FALSE)
```


# Save to Excel

```{r}
#| warning: false
#| message: false
#| label: save-to-excel

# wb <- loadWorkbook("data/outputs/microsimulation_results.xlsx")
# names(wb)
# writeData(
#   wb, 
#   "Poverty", 
#   poor_2023 , 
#   startRow = 3, 
#   startCol = 1, 
#   rowNames = F,
#   na.string = "")
# writeData(
#   wb, 
#   "Poverty", 
#   poor_2030_baseline , 
#   startRow = 3 + (dim(poor_2023)[1] + 3) * 1, 
#   startCol = 1, 
#   rowNames = F,
#   na.string = "")
# writeData(
#   wb, 
#   "Poverty", 
#   poor_2050_baseline , 
#   startRow = 3 + (dim(poor_2023)[1] + 3) * 2, 
#   startCol = 1, 
#   rowNames = F,
#   na.string = "")
# saveWorkbook(
#   wb,
#   "data/outputs/microsimulation_results.xlsx",
#   overwrite = T)
```

# End

```{r}
# test <- poverty |> 
#   group_by(cpsc) |> 
#   summarize(
#     hh_total = sum(weights, na.rm = T),
#     pp_total = sum(weights * hhsize, na.rm =T)
#   )
```